{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA PARAMETERS\n",
    "DATASET_NAME = os.path.join(\"..\", \"data\", \"creditcard.csv\")\n",
    "N_SPLITS = 5\n",
    "\n",
    "# NN PARAMETERS\n",
    "EPOCHS = 120\n",
    "# BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.03\n",
    "NUMBER_OF_NEURONS = 512\n",
    "REGULARIZATION_LAMBDA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UTILS\n",
    "def sigmoid(x):\n",
    "    return np.multiply(0.5, (1 + np.tanh(np.multiply(0.5, x))))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x.clip(min=0)\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return np.multiply(x, (1 - x))\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x > 0] = 1\n",
    "    x[x <= 0] = 0\n",
    "    return x\n",
    "\n",
    "\n",
    "def accuracy_score(actual, predicted):\n",
    "    predicted = predicted.reshape(-1, )\n",
    "    actual = actual.reshape(-1, )\n",
    "\n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "\n",
    "    return (TP + TN) / actual.shape[0]\n",
    "\n",
    "\n",
    "def split_data(dataset, train_size=0.8):\n",
    "    if isinstance(dataset, pd.core.frame.DataFrame):\n",
    "        dataset = dataset.sample(frac=1)\n",
    "    elif isinstance(dataset, np.ndarray):\n",
    "        np.random.shuffle(dataset)\n",
    "    else:\n",
    "        raise TypeError('Argument is invalid! Numpy Array or Pandas DataFrame required.')\n",
    "\n",
    "    size = dataset.shape[0]\n",
    "    return dataset[:int(train_size * size)], dataset[int(train_size * size):]\n",
    "\n",
    "\n",
    "def k_fold_split_data(dataset, k=5):\n",
    "    if isinstance(dataset, np.ndarray):\n",
    "        np.random.shuffle(dataset)\n",
    "    else:\n",
    "        raise TypeError('Argument is invalid! Numpy Array required.')\n",
    "\n",
    "    return np.array_split(dataset, k)\n",
    "\n",
    "\n",
    "def get_under_sample_dataset(dataset, train_and_validation):\n",
    "    fraud_indices = np.array(train_and_validation[train_and_validation.Class == 1].index)\n",
    "    normal_indices = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "    random_normal_indices = np.array(np.random.choice(normal_indices, fraud_indices.shape[0], replace=False))\n",
    "    under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "    return dataset.iloc[under_sample_indices, :].sample(frac=1)\n",
    "\n",
    "def get_radnom_normal_indices(dataset, train_and_validation, amount):\n",
    "    normal_indices = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "    random_normal_indices = np.array(np.random.choice(normal_indices, amount, replace=False))\n",
    "    return dataset.iloc[random_normal_indices, :].sample(frac=1)\n",
    "\n",
    "def plot_loss(epochs, training_history, validation_history):\n",
    "    x_axis = range(0, epochs)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, training_history, label='train_loss')\n",
    "    ax.plot(x_axis, validation_history, label='val_loss')\n",
    "    ax.legend()\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch number')\n",
    "    plt.title('loss vs epoch number')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, x, y):\n",
    "    predicted = np.round(model.predict(x)).reshape(-1, )\n",
    "    actual = y.reshape(-1, )\n",
    "\n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "    FP = np.count_nonzero(np.multiply(predicted, actual - 1))\n",
    "    FN = np.count_nonzero(np.multiply(predicted - 1, actual))\n",
    "\n",
    "    confusion_matrix_dict = {'actual 1': [TP, FN], 'actual 0': [FP, TN]}\n",
    "    confusion_matrix = pd.DataFrame(data=confusion_matrix_dict, columns=['actual 1', 'actual 0'],\n",
    "                                    index=['predicted 1', 'predicted 0'])\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print('\\nPrecision: {}'.format(precision))\n",
    "    print('Recall: {}'.format(recall))\n",
    "    print('F-score: {}'.format(f1))\n",
    "    print('\\n')\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    \n",
    "def plot_ROC(TPR_array, FPR_array):   \n",
    "    plt.title('ROC')\n",
    "    plt.plot(FPR_array, TPR_array, 'b')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([-0.01, 1.01], [-0.01, 1.01],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...         V20       V21       V22  \\\n",
      "0  0.098698  0.363787  0.090794  ...    0.251412 -0.018307  0.277838   \n",
      "1  0.085102 -0.255425 -0.166974  ...   -0.069083 -0.225775 -0.638672   \n",
      "2  0.247676 -1.514654  0.207643  ...    0.524980  0.247998  0.771679   \n",
      "3  0.377436 -1.387024 -0.054952  ...   -0.208038 -0.108300  0.005274   \n",
      "4 -0.270533  0.817739  0.753074  ...    0.408542 -0.009431  0.798278   \n",
      "\n",
      "        V23       V24       V25       V26       V27       V28  Class  \n",
      "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
      "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
      "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
      "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
      "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read CSV\n",
    "dataset = pd.read_csv(DATASET_NAME)\n",
    "\n",
    "assert not dataset.isnull().values.any()\n",
    "#dataset['Amount'] = StandardScaler().fit_transform(dataset[['Amount']])\n",
    "dataset = dataset.drop(['Amount'],axis=1)\n",
    "#dataset['Hour'] = dataset['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)\n",
    "dataset = dataset.drop(['Time'],axis=1)\n",
    "NUMBER_OF_FEATURES = dataset.shape[1] - 1 # Minus 1 because of column: 'Class'\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEXCAYAAACdwyIfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBY+IiFjq/wEs5BITJe0CZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f187d25f908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histogram for all data\n",
    "count_classes = pd.value_counts(dataset['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "count_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset on train_and_validation dataset and test dataset\n",
    "# train_and_validation, test = split_data(dataset, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export train_and_validation and test dataset\n",
    "# pickle.dump( train_and_validation, open(os.path.join(\"..\", \"data\", \"train_and_validation_dataset.data\"), \"wb\" ))\n",
    "# pickle.dump( test, open(os.path.join(\"..\", \"data\", \"test_dataset.data\"), \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train_and_validation dataset\n",
    "train_and_validation = pickle.load(open(os.path.join(\"..\", \"data\", \"train_and_validation_dataset.data\"), \"rb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, neurons_number, learning_rate, activation):\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.output_layer = True\n",
    "\n",
    "        self.input = np.asmatrix(np.zeros((input_dim + 1, 1)))\n",
    "        self.output = np.asmatrix(np.zeros((neurons_number, 1)))\n",
    "        self.weights = np.asmatrix(np.random.uniform(low=-2/(input_dim**0.5), high=2/(input_dim**0.5), size=(input_dim + 1, neurons_number)))\n",
    "        self.deltas = np.asmatrix(np.zeros((neurons_number, 1)))\n",
    "        self.cumulative_gradient = np.asmatrix(np.zeros((input_dim + 1, neurons_number)))\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu(x)\n",
    "\n",
    "    def _get_gradient(self):\n",
    "        return np.matmul(self.input, self.deltas.transpose())\n",
    "\n",
    "    def forward_step(self, input_data):\n",
    "        self.input = np.concatenate([[[1]], input_data])  # Add bias\n",
    "        self.output = self._activate(np.matmul(self.weights.transpose(), self.input))\n",
    "        return self.output\n",
    "\n",
    "    def backward_step(self, next_weights=None, next_deltas=None, output_delta=None):\n",
    "        if self.output_layer:\n",
    "            self.deltas = output_delta\n",
    "        else:\n",
    "            derivative_of_activation = self.get_activation_derivative(self.output)\n",
    "            self.deltas = np.multiply(np.matmul(np.delete(next_weights, 0, 0), next_deltas),\n",
    "                                      derivative_of_activation)  # Exclude bias row from weights\n",
    "\n",
    "        self.cumulative_gradient = self.cumulative_gradient + self._get_gradient()\n",
    "\n",
    "    def get_activation_derivative(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid_derivative(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu_derivative(x)\n",
    "\n",
    "    def get_deltas(self):\n",
    "        return self.deltas\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, batch_size=50, epochs=20, loss='mse', regular_lambda=0.1):\n",
    "        self.layers = []\n",
    "        self.lerning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.regular_lambda = regular_lambda\n",
    "\n",
    "        self.training_history = []\n",
    "        self.validation_history = []\n",
    "\n",
    "    def _global_forward_step(self, x_train_batch):\n",
    "        y_predicted_batch = []\n",
    "        for x_train_record in x_train_batch:\n",
    "            x_train_record = x_train_record.reshape((-1, 1))\n",
    "            for layer in self.layers:\n",
    "                x_train_record = layer.forward_step(x_train_record)\n",
    "            y_predicted_batch.append(x_train_record)\n",
    "\n",
    "        return np.concatenate(y_predicted_batch)\n",
    "\n",
    "    def _global_backward_step(self, y_predicted_record, y_train_record):\n",
    "\n",
    "        output_delta = self._count_output_delta(y_predicted_record, y_train_record)\n",
    "\n",
    "        output_layer = self.layers[-1]\n",
    "        output_layer.backward_step(output_delta=output_delta)\n",
    "\n",
    "        next_deltas = output_layer.get_deltas()\n",
    "        next_weights = output_layer.get_weights()\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            layer.backward_step(next_weights=next_weights, next_deltas=next_deltas)\n",
    "\n",
    "    def _count_output_delta(self, y_predicted, y_actual):\n",
    "        if self.loss == 'mse':\n",
    "            return np.sum(np.multiply((y_predicted - y_actual), self.layers[-1].get_activation_derivative(y_predicted)), axis=0)\n",
    "\n",
    "    def _save_loss(self, x_tr, y_tr, x_val, y_val):\n",
    "        y_predicted_train = self._global_forward_step(x_tr)\n",
    "        train_loss = self._count_loss(y_predicted_train, y_tr)\n",
    "        self.training_history.append(train_loss)\n",
    "\n",
    "        if x_val is not None and y_val is not None:\n",
    "            y_predicted_validation = self._global_forward_step(x_val)\n",
    "            validation_loss = self._count_loss(y_predicted_validation, y_val)\n",
    "            self.validation_history.append(validation_loss)\n",
    "\n",
    "        print(\"Loss: \", train_loss, \" Accuracy: \", accuracy_score(y_tr, np.round(y_predicted_train)))\n",
    "\n",
    "    def _count_loss(self, y_predicted, y_actual):\n",
    "        if self.loss == 'mse':\n",
    "            return np.average(np.square(y_actual - y_predicted))\n",
    "\n",
    "    def add_layer(self, input_dim, neurons_number, activation='sigmoid'):\n",
    "        layer = Layer(input_dim, neurons_number, learning_rate=self.lerning_rate, activation=activation)\n",
    "        if self.layers:\n",
    "            self.layers[-1].output_layer = False\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None, distinct = 0):\n",
    "        print(x_train.shape, y_train.shape)\n",
    "        tr = np.hstack((x_train, y_train))\n",
    "        tr_f = tr[tr[:, -1] == 1]\n",
    "        tr_n = tr[tr[:, -1] == 0]\n",
    "        tr_f_ep = tr_f\n",
    "        tr_n_ep = tr_n\n",
    "        tr_score = np.concatenate((tr_f_ep, tr_n[: tr_f.shape[0],:]))\n",
    "        #print(\"FIT: tyle jest frauds:\", tr_f.shape)\n",
    "        #print(\"FIT: tyle jest normals:\", tr_n.shape)\n",
    "        for i in range(self.epochs):\n",
    "            if(distinct):\n",
    "                tr_n = np.roll(tr_n, tr_f.shape[0])\n",
    "                tr_n_ep = tr_n[: tr_f.shape[0],:]\n",
    "                tr = np.concatenate((tr_f_ep, tr_n_ep))\n",
    "                x_tr = tr[:,:-1]\n",
    "                y_tr = tr[:,-1:]\n",
    "                \n",
    "            else:\n",
    "                x_tr = x_train\n",
    "                y_tr = y_train\n",
    "                \n",
    "            # zeby w kazdej epoce byly inne dobre i te same zle ( w jednej walidacji oczywiscie)\n",
    "            \n",
    "            for idx in range(0, x_tr.shape[0], self.batch_size):\n",
    "                x_train_batch = x_tr[idx:idx + self.batch_size]\n",
    "                y_train_batch = y_tr[idx:idx + self.batch_size]\n",
    "\n",
    "                for n, x_train_record in enumerate(x_train_batch):\n",
    "                    x_train_record = x_train_record.reshape(1, -1)\n",
    "                    y_predicted_record = self._global_forward_step(x_train_record)\n",
    "                    self._global_backward_step(y_predicted_record, y_train_batch[n])\n",
    "\n",
    "                for lyr in self.layers:\n",
    "                    gradient = lyr.cumulative_gradient / x_train_batch.shape[0] + self.regular_lambda * lyr.weights\n",
    "                    lyr.weights = lyr.weights - np.multiply(self.lerning_rate, gradient)\n",
    "                    lyr.cumulative_gradient = np.asmatrix(np.zeros(lyr.cumulative_gradient.shape))\n",
    "            self._save_loss(tr_score[:,:-1], tr_score[:,-1:], x_val, y_val)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self._global_forward_step(x)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        y_predicted = self._global_forward_step(x)\n",
    "        loss = self._count_loss(y_predicted, y)\n",
    "\n",
    "        print(\"Loss: \", loss)\n",
    "        print(\"Accuracy: \", accuracy_score(y, np.round(y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31125849  0.07502777 -1.2928324  ..., -0.02183661  0.02363838  0.        ]\n",
      " [ 1.24590198 -0.43871453  0.88492388 ...,  0.08551132  0.03262617  0.        ]\n",
      " [ 0.14887056  1.05211279 -0.40872921 ...,  0.22376192  0.08451823  0.        ]\n",
      " ..., \n",
      " [ 1.03757211 -0.33327188  1.20322329 ...,  0.04464496  0.0115034   0.        ]\n",
      " [ 2.04540926  0.07184454 -1.81688909 ..., -0.06549266 -0.04496777  0.        ]\n",
      " [-0.37442953  0.44121781  2.01932693 ...,  0.07417494  0.14512983  0.        ]]\n",
      "(227065, 29) (390, 29) (227455, 29)\n"
     ]
    }
   ],
   "source": [
    "#Create array with only bad transactions for test&validation\n",
    "fraud_indices = np.array(train_and_validation[train_and_validation.Class == 1].index)\n",
    "fraud_indices = dataset.iloc[fraud_indices, :].sample(frac=1)\n",
    "fraud_indices = fraud_indices.values\n",
    "\n",
    "kfold_split_frauds = k_fold_split_data(fraud_indices, N_SPLITS)\n",
    "kfold_split_frauds_lens = [len(x) for x in kfold_split_frauds]\n",
    "\n",
    "normal_total = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "normal_total_df = dataset.iloc[normal_total, :].sample(frac=1)\n",
    "normal_total = normal_total_df.values\n",
    "print(normal_total)\n",
    "\n",
    "np.random.shuffle(normal_total)\n",
    "normal_validation_indices = normal_total[:fraud_indices.shape[0], :]\n",
    "normal_train = normal_total[fraud_indices.shape[0]:, :]\n",
    "\n",
    "kfold_split_normals = k_fold_split_data(normal_validation_indices, N_SPLITS)\n",
    "\n",
    "print(normal_train.shape, normal_validation_indices.shape, normal_total.shape)\n",
    "\n",
    "# k-fold validation with k=5\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.541883707066  Accuracy:  0.27724358974358976\n",
      "Loss:  0.516188289587  Accuracy:  0.27724358974358976\n",
      "Loss:  0.438551883819  Accuracy:  0.296474358974359\n",
      "Loss:  0.228181924523  Accuracy:  0.5737179487179487\n",
      "Loss:  0.21021879976  Accuracy:  0.5785256410256411\n",
      "Loss:  0.197392251001  Accuracy:  0.6089743589743589\n",
      "Loss:  0.187834106358  Accuracy:  0.6474358974358975\n",
      "Loss:  0.178879414374  Accuracy:  0.6634615384615384\n",
      "Loss:  0.169338720244  Accuracy:  0.6987179487179487\n",
      "Loss:  0.163119151682  Accuracy:  0.717948717948718\n",
      "Loss:  0.156473244252  Accuracy:  0.7451923076923077\n",
      "Loss:  0.150760714489  Accuracy:  0.7516025641025641\n",
      "Loss:  0.146064792791  Accuracy:  0.7580128205128205\n",
      "Loss:  0.141437544507  Accuracy:  0.780448717948718\n",
      "Loss:  0.137280521461  Accuracy:  0.7900641025641025\n",
      "Loss:  0.133041100709  Accuracy:  0.7932692307692307\n",
      "Loss:  0.129495020573  Accuracy:  0.8125\n",
      "Loss:  0.126905657451  Accuracy:  0.8173076923076923\n",
      "Loss:  0.124791616023  Accuracy:  0.8189102564102564\n",
      "Loss:  0.121225607354  Accuracy:  0.8301282051282052\n",
      "Loss:  0.118183026485  Accuracy:  0.8413461538461539\n",
      "Loss:  0.115622629638  Accuracy:  0.8461538461538461\n",
      "Loss:  0.113304166725  Accuracy:  0.8541666666666666\n",
      "Loss:  0.111040744938  Accuracy:  0.8621794871794872\n",
      "Loss:  0.109176649702  Accuracy:  0.8733974358974359\n",
      "Loss:  0.106899275843  Accuracy:  0.8830128205128205\n",
      "Loss:  0.104299910662  Accuracy:  0.8878205128205128\n",
      "Loss:  0.102430644995  Accuracy:  0.8910256410256411\n",
      "Loss:  0.100787208485  Accuracy:  0.8942307692307693\n",
      "Loss:  0.0993862207086  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0984258601228  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0965817645615  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0951569058986  Accuracy:  0.907051282051282\n",
      "Loss:  0.0940886968219  Accuracy:  0.905448717948718\n",
      "Loss:  0.0934139518876  Accuracy:  0.907051282051282\n",
      "Loss:  0.0927704797631  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0919309103047  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0912909364922  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0906751923544  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0899378832505  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0892754708965  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0885097982644  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0878985571171  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0871338881268  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0866319754791  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0860536492021  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0858608276995  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0853588508148  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0848623864063  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0845479199689  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0840999086716  Accuracy:  0.9134615384615384\n",
      "Loss:  0.083510338102  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0829864750419  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0824779733555  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0822100606289  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0817922687657  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0815401799333  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0811429494636  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0809366525489  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0808001315692  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0804781045504  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0802007934606  Accuracy:  0.9150641025641025\n",
      "Loss:  0.080013765262  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0799211963662  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0795363867464  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0791988982358  Accuracy:  0.9166666666666666\n",
      "Loss:  0.078901354854  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0788199237456  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0784715050541  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0782665672533  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0780623468826  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0777920801983  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0775261883452  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0774207719294  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0770140301942  Accuracy:  0.9198717948717948\n",
      "Loss:  0.076931637182  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0766729592247  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0764496780472  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0763770406013  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0763639526288  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0762153682716  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0760392760124  Accuracy:  0.9166666666666666\n",
      "Loss:  0.075947503893  Accuracy:  0.9166666666666666\n",
      "Loss:  0.07581992289  Accuracy:  0.9166666666666666\n",
      "Loss:  0.075686676938  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0754259819355  Accuracy:  0.9150641025641025\n",
      "Loss:  0.075224462612  Accuracy:  0.9150641025641025\n",
      "Loss:  0.074852301288  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0747062617331  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0746468630622  Accuracy:  0.9182692307692307\n",
      "Loss:  0.074540059984  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0744116128077  Accuracy:  0.9166666666666666\n",
      "Loss:  0.074151835004  Accuracy:  0.9166666666666666\n",
      "Loss:  0.074080552268  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0739577213732  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0739451645212  Accuracy:  0.9166666666666666\n",
      "Loss:  0.07389814346  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0738145223127  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0737667682471  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0735590214426  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0734162679643  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0734119863357  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0733372763491  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0734335226089  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0734108840658  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0734427522391  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0733512126654  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0734711707003  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0732548553633  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0731902856643  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0731742997756  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0729915950284  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0730488458258  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0729177865626  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0727791112338  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0726817053094  Accuracy:  0.9182692307692307\n",
      "Loss:  0.072521725043  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0724068779704  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0724253579629  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0723972508324  Accuracy:  0.9198717948717948\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0767777744885\n",
      "Accuracy:  0.9230769230769231\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.454489792104  Accuracy:  0.40064102564102566\n",
      "Loss:  0.284789679607  Accuracy:  0.5448717948717948\n",
      "Loss:  0.177055518113  Accuracy:  0.7019230769230769\n",
      "Loss:  0.16881114969  Accuracy:  0.717948717948718\n",
      "Loss:  0.161996642904  Accuracy:  0.7339743589743589\n",
      "Loss:  0.158372811693  Accuracy:  0.7403846153846154\n",
      "Loss:  0.15115683328  Accuracy:  0.7548076923076923\n",
      "Loss:  0.145498147142  Accuracy:  0.7676282051282052\n",
      "Loss:  0.142347792414  Accuracy:  0.7772435897435898\n",
      "Loss:  0.136733830092  Accuracy:  0.7980769230769231\n",
      "Loss:  0.132725940415  Accuracy:  0.8141025641025641\n",
      "Loss:  0.128778211395  Accuracy:  0.8285256410256411\n",
      "Loss:  0.125141726802  Accuracy:  0.8397435897435898\n",
      "Loss:  0.123060749058  Accuracy:  0.842948717948718\n",
      "Loss:  0.119571701551  Accuracy:  0.844551282051282\n",
      "Loss:  0.117013451453  Accuracy:  0.8525641025641025\n",
      "Loss:  0.114745848981  Accuracy:  0.8573717948717948\n",
      "Loss:  0.111766117514  Accuracy:  0.8701923076923077\n",
      "Loss:  0.109719203514  Accuracy:  0.8733974358974359\n",
      "Loss:  0.10756206097  Accuracy:  0.8782051282051282\n",
      "Loss:  0.105092919266  Accuracy:  0.8814102564102564\n",
      "Loss:  0.103266117895  Accuracy:  0.8814102564102564\n",
      "Loss:  0.102048528685  Accuracy:  0.8814102564102564\n",
      "Loss:  0.100600010538  Accuracy:  0.8830128205128205\n",
      "Loss:  0.0997501146513  Accuracy:  0.8862179487179487\n",
      "Loss:  0.0988080013281  Accuracy:  0.8910256410256411\n",
      "Loss:  0.0971121882  Accuracy:  0.8926282051282052\n",
      "Loss:  0.0962171216549  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0948517480385  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0931313766198  Accuracy:  0.9102564102564102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0919829802071  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0913718965808  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0905180498485  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0896557787262  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0888417803085  Accuracy:  0.9262820512820513\n",
      "Loss:  0.087758468839  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0871035063567  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0862780135122  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0855231795183  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0846672227431  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0841256569046  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0834150998891  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0827873571438  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0821927240482  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0819342246741  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0816230918855  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0815271237199  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0809503049813  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0802729296958  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0795225220955  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0790198144594  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0786172586253  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0782875477047  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0777521800567  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0773811306238  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0769650050868  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0766713658349  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0763234243201  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0759914196402  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0756764297276  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0753790590225  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0751265846997  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0750722343484  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0748705886509  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0745348112447  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0743395366279  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0742287681049  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0740877689388  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0737783994718  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0735878821232  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0733592990683  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0731248694273  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0728917124466  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0726438747869  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0724838139714  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0722829254959  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0720760356247  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0719062027126  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0718087379501  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0716840741393  Accuracy:  0.9375\n",
      "Loss:  0.0716575981213  Accuracy:  0.9375\n",
      "Loss:  0.0714548069509  Accuracy:  0.9375\n",
      "Loss:  0.0714217460423  Accuracy:  0.9375\n",
      "Loss:  0.0713356263425  Accuracy:  0.9375\n",
      "Loss:  0.0711841134872  Accuracy:  0.9375\n",
      "Loss:  0.071009601935  Accuracy:  0.9375\n",
      "Loss:  0.0708545753528  Accuracy:  0.9375\n",
      "Loss:  0.0707861563937  Accuracy:  0.9375\n",
      "Loss:  0.0706262222998  Accuracy:  0.9375\n",
      "Loss:  0.070484723779  Accuracy:  0.9375\n",
      "Loss:  0.070391541677  Accuracy:  0.9375\n",
      "Loss:  0.070210523273  Accuracy:  0.9375\n",
      "Loss:  0.0700842374631  Accuracy:  0.9375\n",
      "Loss:  0.0699855663679  Accuracy:  0.9375\n",
      "Loss:  0.0699352310918  Accuracy:  0.9375\n",
      "Loss:  0.0698925085778  Accuracy:  0.9375\n",
      "Loss:  0.0698432197988  Accuracy:  0.9375\n",
      "Loss:  0.0696925959516  Accuracy:  0.9375\n",
      "Loss:  0.0695987764253  Accuracy:  0.9375\n",
      "Loss:  0.0694106931561  Accuracy:  0.9375\n",
      "Loss:  0.0692676221984  Accuracy:  0.9375\n",
      "Loss:  0.0691326786404  Accuracy:  0.9375\n",
      "Loss:  0.0690770234904  Accuracy:  0.9375\n",
      "Loss:  0.0691300785046  Accuracy:  0.9375\n",
      "Loss:  0.069105073278  Accuracy:  0.9375\n",
      "Loss:  0.0689997999228  Accuracy:  0.9375\n",
      "Loss:  0.0689472171381  Accuracy:  0.9375\n",
      "Loss:  0.0688249225883  Accuracy:  0.9375\n",
      "Loss:  0.0687772608001  Accuracy:  0.9375\n",
      "Loss:  0.0687420614982  Accuracy:  0.9375\n",
      "Loss:  0.0686473725697  Accuracy:  0.9375\n",
      "Loss:  0.0687001902133  Accuracy:  0.9375\n",
      "Loss:  0.0686928735547  Accuracy:  0.9375\n",
      "Loss:  0.0686019417019  Accuracy:  0.9375\n",
      "Loss:  0.0685598375289  Accuracy:  0.9375\n",
      "Loss:  0.0684519632722  Accuracy:  0.9375\n",
      "Loss:  0.0683563929333  Accuracy:  0.9375\n",
      "Loss:  0.0683978266708  Accuracy:  0.9375\n",
      "Loss:  0.0683799305573  Accuracy:  0.9375\n",
      "Loss:  0.0683582757776  Accuracy:  0.9375\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0653649940163\n",
      "Accuracy:  0.9423076923076923\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.210985607574  Accuracy:  0.5865384615384616\n",
      "Loss:  0.204214406669  Accuracy:  0.6025641025641025\n",
      "Loss:  0.195154333513  Accuracy:  0.6201923076923077\n",
      "Loss:  0.187191847423  Accuracy:  0.6442307692307693\n",
      "Loss:  0.18151990741  Accuracy:  0.6666666666666666\n",
      "Loss:  0.174281514147  Accuracy:  0.6875\n",
      "Loss:  0.166256467585  Accuracy:  0.7099358974358975\n",
      "Loss:  0.159698101738  Accuracy:  0.7371794871794872\n",
      "Loss:  0.153610358341  Accuracy:  0.7692307692307693\n",
      "Loss:  0.147868964351  Accuracy:  0.7836538461538461\n",
      "Loss:  0.143294794564  Accuracy:  0.7932692307692307\n",
      "Loss:  0.138669304378  Accuracy:  0.8028846153846154\n",
      "Loss:  0.134674337997  Accuracy:  0.8125\n",
      "Loss:  0.130256078369  Accuracy:  0.8253205128205128\n",
      "Loss:  0.126375468945  Accuracy:  0.8397435897435898\n",
      "Loss:  0.122911760202  Accuracy:  0.844551282051282\n",
      "Loss:  0.11928349158  Accuracy:  0.8589743589743589\n",
      "Loss:  0.115932959384  Accuracy:  0.8669871794871795\n",
      "Loss:  0.113458307876  Accuracy:  0.8669871794871795\n",
      "Loss:  0.111324418703  Accuracy:  0.8733974358974359\n",
      "Loss:  0.109426403038  Accuracy:  0.875\n",
      "Loss:  0.107170841914  Accuracy:  0.8814102564102564\n",
      "Loss:  0.104993862148  Accuracy:  0.8894230769230769\n",
      "Loss:  0.103411520137  Accuracy:  0.8926282051282052\n",
      "Loss:  0.102072985474  Accuracy:  0.8958333333333334\n",
      "Loss:  0.100690355608  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0989841134401  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0976683831335  Accuracy:  0.8990384615384616\n",
      "Loss:  0.096258307361  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0943699754329  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0933805813823  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0924479041804  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0915002195065  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0906816988672  Accuracy:  0.9150641025641025\n",
      "Loss:  0.089777144533  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0889842672548  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0881505148772  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0875293597308  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0867287809943  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0861241129431  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0855610874809  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0850256024186  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0840060219123  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0832635890426  Accuracy:  0.9246794871794872\n",
      "Loss:  0.082702600942  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0823175437732  Accuracy:  0.9262820512820513\n",
      "Loss:  0.081631118028  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0811487447431  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0807466377236  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0804160538264  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0796717435756  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0791408668738  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0789115081237  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0783076380214  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0779383723717  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0775314254569  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0771683467957  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0767191481867  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0763559504861  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0760194645516  Accuracy:  0.9262820512820513\n",
      "Loss:  0.075783157044  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0754836211653  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0751469258781  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0747132314399  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0744240536535  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0741526763441  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0739493238641  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0737198908435  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0735031324304  Accuracy:  0.9278846153846154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0733983358723  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0732517772796  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729735162877  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0727496013171  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0725238271617  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0723146588263  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0720745280502  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0719527822795  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0717914786746  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0716988770255  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0715529152364  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0714137844943  Accuracy:  0.9375\n",
      "Loss:  0.0712751754201  Accuracy:  0.9375\n",
      "Loss:  0.0710662420856  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0708770424243  Accuracy:  0.9342948717948718\n",
      "Loss:  0.070742004151  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0706096359194  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0704261248067  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0702248374567  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0701472992286  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0700784734573  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0699618784638  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0699086130455  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0697647891842  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0695787851782  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0695268228452  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0695612912952  Accuracy:  0.9358974358974359\n",
      "Loss:  0.069479072794  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0694363533843  Accuracy:  0.9358974358974359\n",
      "Loss:  0.069404801407  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0692408869544  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0691084724893  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0690921344094  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0689913206594  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0689632265968  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0688767796107  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0688516089369  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0687668086687  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0686520177038  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0684967411201  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0685386204864  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0684400304656  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0683199315228  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0682857930335  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0682270967969  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0681932839551  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0681347110864  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0681205112712  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0680570047076  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0680149694241  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0679809574826  Accuracy:  0.9358974358974359\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0659868950964\n",
      "Accuracy:  0.9487179487179487\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.232033463099  Accuracy:  0.5544871794871795\n",
      "Loss:  0.21979525868  Accuracy:  0.5705128205128205\n",
      "Loss:  0.207077932439  Accuracy:  0.6073717948717948\n",
      "Loss:  0.196661783274  Accuracy:  0.6282051282051282\n",
      "Loss:  0.186627354613  Accuracy:  0.6426282051282052\n",
      "Loss:  0.175841105456  Accuracy:  0.6858974358974359\n",
      "Loss:  0.166716618687  Accuracy:  0.7099358974358975\n",
      "Loss:  0.158663356465  Accuracy:  0.7355769230769231\n",
      "Loss:  0.151715361108  Accuracy:  0.7580128205128205\n",
      "Loss:  0.147626980217  Accuracy:  0.7628205128205128\n",
      "Loss:  0.14226483132  Accuracy:  0.7756410256410257\n",
      "Loss:  0.13665570847  Accuracy:  0.7916666666666666\n",
      "Loss:  0.132463205127  Accuracy:  0.8076923076923077\n",
      "Loss:  0.128916745081  Accuracy:  0.8205128205128205\n",
      "Loss:  0.124691381228  Accuracy:  0.8381410256410257\n",
      "Loss:  0.120934780046  Accuracy:  0.844551282051282\n",
      "Loss:  0.117059749112  Accuracy:  0.8589743589743589\n",
      "Loss:  0.1143919569  Accuracy:  0.8653846153846154\n",
      "Loss:  0.111804378243  Accuracy:  0.8717948717948718\n",
      "Loss:  0.109927095927  Accuracy:  0.8717948717948718\n",
      "Loss:  0.107424771709  Accuracy:  0.8798076923076923\n",
      "Loss:  0.105827801925  Accuracy:  0.8862179487179487\n",
      "Loss:  0.103556369649  Accuracy:  0.8910256410256411\n",
      "Loss:  0.102048713807  Accuracy:  0.8926282051282052\n",
      "Loss:  0.101110702939  Accuracy:  0.8974358974358975\n",
      "Loss:  0.10021162787  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0985081445125  Accuracy:  0.905448717948718\n",
      "Loss:  0.096754634174  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0952413594428  Accuracy:  0.907051282051282\n",
      "Loss:  0.0934375983885  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0922638043939  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0911168822627  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0901516147913  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0900107918786  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0889181230594  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0879579626105  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0870679051815  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0865054553919  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0859393249585  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0854103740305  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0847659079088  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0843079661658  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0834703988352  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0828765516409  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0824774696849  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0818558154896  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0819666708607  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0816542376728  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0811288245481  Accuracy:  0.9230769230769231\n",
      "Loss:  0.080700981114  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0802267894503  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0796858828049  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0799024976039  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0793855000809  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0787777949834  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0784943126314  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0781483047007  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0776567699617  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0775129606269  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0773086310971  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0770311034027  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0768212409594  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0761589961133  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0758079025971  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0757418983989  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0755640459599  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0760742994774  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0755260358969  Accuracy:  0.9278846153846154\n",
      "Loss:  0.074943206625  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0746357904974  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0746031511085  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0742781530473  Accuracy:  0.9246794871794872\n",
      "Loss:  0.07414329264  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0740171622725  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0737149266439  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0738833502693  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0735803488585  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0734071169468  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0733088852653  Accuracy:  0.9278846153846154\n",
      "Loss:  0.073150209437  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0728802780559  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0726502968569  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0724887924182  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0722255604728  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0719867531343  Accuracy:  0.9246794871794872\n",
      "Loss:  0.071736177778  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0715878904718  Accuracy:  0.9246794871794872\n",
      "Loss:  0.071484763203  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0712073682159  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0711688316753  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0711309838678  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0711044895602  Accuracy:  0.9278846153846154\n",
      "Loss:  0.071096491384  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0709111547358  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0707223207396  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0705246221391  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0704274963009  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0703062164874  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0702100410632  Accuracy:  0.9246794871794872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.070172338849  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0699708050629  Accuracy:  0.9262820512820513\n",
      "Loss:  0.069774573299  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0697008733923  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0697692204563  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0697447744803  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0697316185562  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0697425754884  Accuracy:  0.9278846153846154\n",
      "Loss:  0.069809890885  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0699377943206  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0698152140408  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0697801513926  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0698713167528  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0698595688124  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0697043809342  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0697080780133  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0696595763506  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0695709165528  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0695594861289  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0695279017604  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0695402565299  Accuracy:  0.9278846153846154\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0684799485192\n",
      "Accuracy:  0.9487179487179487\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.184841103506  Accuracy:  0.7035256410256411\n",
      "Loss:  0.175166097528  Accuracy:  0.7323717948717948\n",
      "Loss:  0.172480321463  Accuracy:  0.7451923076923077\n",
      "Loss:  0.16472890171  Accuracy:  0.7644230769230769\n",
      "Loss:  0.159097112881  Accuracy:  0.780448717948718\n",
      "Loss:  0.154081821795  Accuracy:  0.7932692307692307\n",
      "Loss:  0.149061596288  Accuracy:  0.8108974358974359\n",
      "Loss:  0.144033604909  Accuracy:  0.8269230769230769\n",
      "Loss:  0.138590074389  Accuracy:  0.8413461538461539\n",
      "Loss:  0.134154903258  Accuracy:  0.8525641025641025\n",
      "Loss:  0.130582876529  Accuracy:  0.8589743589743589\n",
      "Loss:  0.127411877811  Accuracy:  0.8637820512820513\n",
      "Loss:  0.12393631581  Accuracy:  0.8717948717948718\n",
      "Loss:  0.120885597227  Accuracy:  0.875\n",
      "Loss:  0.118182288508  Accuracy:  0.8798076923076923\n",
      "Loss:  0.115760282093  Accuracy:  0.8830128205128205\n",
      "Loss:  0.113465086113  Accuracy:  0.8814102564102564\n",
      "Loss:  0.110932128542  Accuracy:  0.8862179487179487\n",
      "Loss:  0.108731844106  Accuracy:  0.8894230769230769\n",
      "Loss:  0.106887580742  Accuracy:  0.8910256410256411\n",
      "Loss:  0.104858631839  Accuracy:  0.8926282051282052\n",
      "Loss:  0.102589984519  Accuracy:  0.8942307692307693\n",
      "Loss:  0.100807413538  Accuracy:  0.8942307692307693\n",
      "Loss:  0.102676091095  Accuracy:  0.8926282051282052\n",
      "Loss:  0.0994113736466  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0978130896102  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0962570279413  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0946115153283  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0933171462838  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0921320659128  Accuracy:  0.907051282051282\n",
      "Loss:  0.091182898164  Accuracy:  0.907051282051282\n",
      "Loss:  0.0916093766124  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0910445791894  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0901580877234  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0882738754033  Accuracy:  0.907051282051282\n",
      "Loss:  0.087010537842  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0861801385833  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0858262897666  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0851388919189  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0845715328832  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0840618877113  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0832897889421  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0825949792816  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0820386016163  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0815739497803  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0812271458125  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0807435834547  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0804995400263  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0800921302262  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0795493441889  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0791723736322  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0786346963648  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0780098919155  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0775042842773  Accuracy:  0.9118589743589743\n",
      "Loss:  0.077356435119  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0767686774136  Accuracy:  0.9150641025641025\n",
      "Loss:  0.076138800036  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0757729961573  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0755055104755  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0751409012221  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0748125774385  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0745653677186  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0742256115001  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0739398009805  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0736535651813  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0732451760834  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0728931291653  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0723531188198  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0717796890037  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0710346694748  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0694784467413  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0677018539446  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0665408965293  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0660838805171  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0660101143449  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0658558509792  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0656423479939  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0654470405773  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0652968745189  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0651832388592  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0650451899191  Accuracy:  0.9262820512820513\n",
      "Loss:  0.064903554413  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0649966991562  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0648619151843  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0647461864935  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0645171684606  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0643869224912  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0642399757576  Accuracy:  0.9310897435897436\n",
      "Loss:  0.064212439902  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0640476973796  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0639551713084  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0638415268584  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0637703666294  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0637025577705  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0636880752595  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0637244707975  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0636834583434  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0636248035272  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0636203576031  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0636518739705  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0636255379482  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0636190026843  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0636091128842  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0635789549053  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0634413982581  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0633566637806  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0633017068523  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0633201533493  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0633210643874  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0634138062761  Accuracy:  0.9375\n",
      "Loss:  0.0633642178683  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0634731075328  Accuracy:  0.9375\n",
      "Loss:  0.0634731800441  Accuracy:  0.9375\n",
      "Loss:  0.0634508855057  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0634625948609  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0634322221326  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0632962426185  Accuracy:  0.9375\n",
      "Loss:  0.0633980792515  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0634309879791  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0634039862878  Accuracy:  0.9358974358974359\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0804827656355\n",
      "Accuracy:  0.8910256410256411\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, validation_frauds in enumerate(kfold_split_frauds):\n",
    "    #Get train frauds from train&validation frauds\n",
    "    train_frauds = kfold_split_frauds.copy()\n",
    "    train_frauds.pop(k)\n",
    "    train_frauds = np.concatenate(train_frauds)\n",
    "    print(len(train_frauds))\n",
    "    \n",
    "    validation_normals = kfold_split_normals[k]\n",
    "    np.random.shuffle(normal_train)\n",
    "    train_normals = normal_train[:train_frauds.shape[0]*EPOCHS,:]\n",
    "    \n",
    "    train = np.concatenate([train_frauds, train_normals])\n",
    "    \n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1:]\n",
    "    \n",
    "    validation = np.concatenate([validation_frauds,validation_normals])\n",
    "    np.random.shuffle(validation)\n",
    "\n",
    "    x_validation = validation[:, :-1]\n",
    "    y_validation = validation[:, -1:]\n",
    "\n",
    "    model = NeuralNetwork(learning_rate=LEARNING_RATE, batch_size=x_train.shape[0], epochs=EPOCHS, loss='mse', regular_lambda=REGULARIZATION_LAMBDA)\n",
    "    model.add_layer(input_dim=NUMBER_OF_FEATURES, neurons_number=NUMBER_OF_NEURONS, activation='relu')\n",
    "    model.add_layer(input_dim=NUMBER_OF_NEURONS, neurons_number=1, activation='sigmoid')\n",
    "    model.fit(x_train, y_train, x_validation, y_validation, distinct=1)\n",
    "\n",
    "    print(\"\\n\", '='*80)\n",
    "    print(\"\\nValidation dataset evaluation:\")\n",
    "    model.evaluate(x_validation, y_validation)\n",
    "    print(\"\\n\")\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEaCAYAAADpMdsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nigchhJCyUfEghBBSNioehBBCyvYdYQTLtaSBDfYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f187dadf198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history = np.average([mdl.training_history for mdl in models], axis=0)\n",
    "validation_history = np.average([mdl.validation_history for mdl in models], axis=0)\n",
    "\n",
    "plot_loss(EPOCHS, training_history, validation_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION ON TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset evaluation:\n"
     ]
    }
   ],
   "source": [
    "# Read test dataset\n",
    "test = pickle.load(open(os.path.join(\"..\", \"data\", \"test_dataset.data\"), \"rb\" ))\n",
    "\n",
    "# Convert test data to numpyarray and split them.\n",
    "test = test.values\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1:]\n",
    "\n",
    "print(\"\\nTest dataset evaluation:\")\n",
    "train_and_validation = train_and_validation.values \n",
    "x_train_and_validation = train_and_validation[:, :-1]\n",
    "y_train_and_validation = train_and_validation[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 28) (227845, 1)\n",
      "Loss:  0.205191044803  Accuracy:  0.6166666666666667\n",
      "Loss:  0.190596049043  Accuracy:  0.6487179487179487\n",
      "Loss:  0.18045565901  Accuracy:  0.6730769230769231\n",
      "Loss:  0.171737037605  Accuracy:  0.6987179487179487\n",
      "Loss:  0.163402215232  Accuracy:  0.7256410256410256\n",
      "Loss:  0.156077785424  Accuracy:  0.7461538461538462\n",
      "Loss:  0.148868018316  Accuracy:  0.7653846153846153\n",
      "Loss:  0.142988636056  Accuracy:  0.782051282051282\n",
      "Loss:  0.138040872812  Accuracy:  0.7974358974358975\n",
      "Loss:  0.132938639916  Accuracy:  0.8115384615384615\n",
      "Loss:  0.128046120379  Accuracy:  0.8243589743589743\n",
      "Loss:  0.123229792232  Accuracy:  0.8320512820512821\n",
      "Loss:  0.119604817246  Accuracy:  0.8435897435897436\n",
      "Loss:  0.116027934025  Accuracy:  0.85\n",
      "Loss:  0.112915471376  Accuracy:  0.8615384615384616\n",
      "Loss:  0.110011840147  Accuracy:  0.8782051282051282\n",
      "Loss:  0.1069778476  Accuracy:  0.8794871794871795\n",
      "Loss:  0.104483985875  Accuracy:  0.8858974358974359\n",
      "Loss:  0.102100342893  Accuracy:  0.8897435897435897\n",
      "Loss:  0.100648600388  Accuracy:  0.8961538461538462\n",
      "Loss:  0.099038293587  Accuracy:  0.8987179487179487\n",
      "Loss:  0.0968312097844  Accuracy:  0.8961538461538462\n",
      "Loss:  0.095045927842  Accuracy:  0.9\n",
      "Loss:  0.0937313597752  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0924063754456  Accuracy:  0.9076923076923077\n",
      "Loss:  0.0909443040411  Accuracy:  0.9076923076923077\n",
      "Loss:  0.0900175900229  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0891418997714  Accuracy:  0.9153846153846154\n",
      "Loss:  0.0880582416361  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0873414663233  Accuracy:  0.9192307692307692\n",
      "Loss:  0.0862161591143  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0855876190224  Accuracy:  0.9243589743589744\n",
      "Loss:  0.084790915  Accuracy:  0.9256410256410257\n",
      "Loss:  0.0841291451682  Accuracy:  0.926923076923077\n",
      "Loss:  0.083307349573  Accuracy:  0.926923076923077\n",
      "Loss:  0.0827062261241  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0819828085324  Accuracy:  0.926923076923077\n",
      "Loss:  0.0815072572255  Accuracy:  0.926923076923077\n",
      "Loss:  0.0808396106475  Accuracy:  0.926923076923077\n",
      "Loss:  0.0801890496005  Accuracy:  0.926923076923077\n",
      "Loss:  0.0795242731098  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0789147726688  Accuracy:  0.926923076923077\n",
      "Loss:  0.0784123005357  Accuracy:  0.926923076923077\n",
      "Loss:  0.0779366952515  Accuracy:  0.926923076923077\n",
      "Loss:  0.0775285778569  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0770404301231  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0766881449439  Accuracy:  0.9294871794871795\n",
      "Loss:  0.076286277945  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0765040865075  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0762529261837  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0762331438961  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0758245939472  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0755424411003  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0753237801496  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0753756679366  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0751067844284  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0747817343977  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0744679539754  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0742255847819  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0739408345022  Accuracy:  0.9307692307692308\n",
      "Loss:  0.073656406461  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0733734809401  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0732614566109  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0730939958297  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0729169023962  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0726919861306  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0725148248027  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0723186382904  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0721775451167  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0720268411072  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0720389085267  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0720417040983  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0719327562536  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0717529057152  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0716359855234  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0715151946585  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0714046932943  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0710248121266  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0708935622375  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0706753816637  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0704703679438  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0703969285873  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0719280480947  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0708556470844  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0706068814806  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0703705340724  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0701712330913  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0699252345643  Accuracy:  0.926923076923077\n",
      "Loss:  0.0699382026787  Accuracy:  0.926923076923077\n",
      "Loss:  0.0698828452524  Accuracy:  0.926923076923077\n",
      "Loss:  0.0699243380485  Accuracy:  0.926923076923077\n",
      "Loss:  0.0698793289228  Accuracy:  0.926923076923077\n",
      "Loss:  0.0697508591898  Accuracy:  0.926923076923077\n",
      "Loss:  0.069727695701  Accuracy:  0.926923076923077\n",
      "Loss:  0.0696132231548  Accuracy:  0.926923076923077\n",
      "Loss:  0.0695696050923  Accuracy:  0.926923076923077\n",
      "Loss:  0.0694465996496  Accuracy:  0.926923076923077\n",
      "Loss:  0.0694335611822  Accuracy:  0.926923076923077\n",
      "Loss:  0.0693311121975  Accuracy:  0.926923076923077\n",
      "Loss:  0.069337383331  Accuracy:  0.926923076923077\n",
      "Loss:  0.0693180459665  Accuracy:  0.926923076923077\n",
      "Loss:  0.0692235427691  Accuracy:  0.926923076923077\n",
      "Loss:  0.0691350388009  Accuracy:  0.926923076923077\n",
      "Loss:  0.0691267838965  Accuracy:  0.926923076923077\n",
      "Loss:  0.0691017120428  Accuracy:  0.926923076923077\n",
      "Loss:  0.0691272238663  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0691400199366  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0690864285108  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0690138921754  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0690191096197  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0691085448872  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0690747894257  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0691329780809  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0690817602291  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0690322558367  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0689766414077  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0689266430422  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0689267205778  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0688953663433  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0689464557007  Accuracy:  0.9294871794871795\n"
     ]
    }
   ],
   "source": [
    "test_model = NeuralNetwork(learning_rate=LEARNING_RATE, batch_size=x_train_and_validation.shape[0], epochs=EPOCHS, loss='mse', regular_lambda=REGULARIZATION_LAMBDA)\n",
    "test_model.add_layer(input_dim=NUMBER_OF_FEATURES, neurons_number=NUMBER_OF_NEURONS, activation='relu')\n",
    "test_model.add_layer(input_dim=NUMBER_OF_NEURONS, neurons_number=1, activation='sigmoid')\n",
    "test_model.fit(x_train_and_validation, y_train_and_validation, distinct=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0732458343604\n",
      "Accuracy:  0.978792879463502\n",
      "\n",
      "Precision: 0.08773584905660377\n",
      "Recall: 0.9117647058823529\n",
      "F-score: 0.16006884681583478\n",
      "\n",
      "\n",
      "             actual 1  actual 0\n",
      "predicted 1        93       967\n",
      "predicted 0         9     55893\n"
     ]
    }
   ],
   "source": [
    "test_model.evaluate(x_test, y_test)\n",
    "plot_confusion_matrix(model, x_test, y_test)\n",
    "# print(model.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:20<00:00,  3.83s/it]\n",
      "/home/maciejpesko/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEaCAYAAADkL6tQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18b4399d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TPR_array = []\n",
    "FPR_array = []\n",
    "for i in tqdm(range(-50,55,5)):\n",
    "    predicted = np.round(model.predict(x_test) + i/100).reshape(y_test.shape)\n",
    "    actual = y_test  \n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "    FP = np.count_nonzero(np.multiply(predicted, actual - 1))\n",
    "    FN = np.count_nonzero(np.multiply(predicted - 1, actual))\n",
    "\n",
    "    TPR_array.append(TP / (TP+FN))\n",
    "    FPR_array.append(FP / (FP+TN))\n",
    "\n",
    "plot_ROC(TPR_array, FPR_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
