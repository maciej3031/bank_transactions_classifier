{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA PARAMETERS\n",
    "DATASET_NAME = os.path.join(\"..\", \"data\", \"creditcard.csv\")\n",
    "N_SPLITS = 5\n",
    "\n",
    "# NN PARAMETERS\n",
    "EPOCHS = 120\n",
    "# BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.03\n",
    "NUMBER_OF_NEURONS = 512\n",
    "REGULARIZATION_LAMBDA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UTILS\n",
    "def sigmoid(x):\n",
    "    return np.multiply(0.5, (1 + np.tanh(np.multiply(0.5, x))))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x.clip(min=0)\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return np.multiply(x, (1 - x))\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x > 0] = 1\n",
    "    x[x <= 0] = 0\n",
    "    return x\n",
    "\n",
    "\n",
    "def accuracy_score(actual, predicted):\n",
    "    predicted = predicted.reshape(-1, )\n",
    "    actual = actual.reshape(-1, )\n",
    "\n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "\n",
    "    return (TP + TN) / actual.shape[0]\n",
    "\n",
    "\n",
    "def split_data(dataset, train_size=0.8):\n",
    "    if isinstance(dataset, pd.core.frame.DataFrame):\n",
    "        dataset = dataset.sample(frac=1)\n",
    "    elif isinstance(dataset, np.ndarray):\n",
    "        np.random.shuffle(dataset)\n",
    "    else:\n",
    "        raise TypeError('Argument is invalid! Numpy Array or Pandas DataFrame required.')\n",
    "\n",
    "    size = dataset.shape[0]\n",
    "    return dataset[:int(train_size * size)], dataset[int(train_size * size):]\n",
    "\n",
    "\n",
    "def k_fold_split_data(dataset, k=5):\n",
    "    if isinstance(dataset, np.ndarray):\n",
    "        np.random.shuffle(dataset)\n",
    "    else:\n",
    "        raise TypeError('Argument is invalid! Numpy Array required.')\n",
    "\n",
    "    return np.array_split(dataset, k)\n",
    "\n",
    "\n",
    "def get_under_sample_dataset(dataset, train_and_validation):\n",
    "    fraud_indices = np.array(train_and_validation[train_and_validation.Class == 1].index)\n",
    "    normal_indices = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "    random_normal_indices = np.array(np.random.choice(normal_indices, fraud_indices.shape[0], replace=False))\n",
    "    under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "    return dataset.iloc[under_sample_indices, :].sample(frac=1)\n",
    "\n",
    "def get_radnom_normal_indices(dataset, train_and_validation, amount):\n",
    "    normal_indices = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "    random_normal_indices = np.array(np.random.choice(normal_indices, amount, replace=False))\n",
    "    return dataset.iloc[random_normal_indices, :].sample(frac=1)\n",
    "\n",
    "def plot_loss(epochs, training_history, validation_history):\n",
    "    x_axis = range(0, epochs)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, training_history, label='train_loss')\n",
    "    ax.plot(x_axis, validation_history, label='val_loss')\n",
    "    ax.legend()\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch number')\n",
    "    plt.title('loss vs epoch number')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, x, y):\n",
    "    predicted = np.round(model.predict(x)).reshape(-1, )\n",
    "    actual = y.reshape(-1, )\n",
    "\n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "    FP = np.count_nonzero(np.multiply(predicted, actual - 1))\n",
    "    FN = np.count_nonzero(np.multiply(predicted - 1, actual))\n",
    "\n",
    "    confusion_matrix_dict = {'actual 1': [TP, FN], 'actual 0': [FP, TN]}\n",
    "    confusion_matrix = pd.DataFrame(data=confusion_matrix_dict, columns=['actual 1', 'actual 0'],\n",
    "                                    index=['predicted 1', 'predicted 0'])\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print('\\nPrecision: {}'.format(precision))\n",
    "    print('Recall: {}'.format(recall))\n",
    "    print('F-score: {}'.format(f1))\n",
    "    print('\\n')\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    \n",
    "def plot_ROC(TPR_array, FPR_array):   \n",
    "    plt.title('ROC')\n",
    "    plt.plot(FPR_array, TPR_array, 'b')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...         V20       V21       V22  \\\n",
      "0  0.098698  0.363787  0.090794  ...    0.251412 -0.018307  0.277838   \n",
      "1  0.085102 -0.255425 -0.166974  ...   -0.069083 -0.225775 -0.638672   \n",
      "2  0.247676 -1.514654  0.207643  ...    0.524980  0.247998  0.771679   \n",
      "3  0.377436 -1.387024 -0.054952  ...   -0.208038 -0.108300  0.005274   \n",
      "4 -0.270533  0.817739  0.753074  ...    0.408542 -0.009431  0.798278   \n",
      "\n",
      "        V23       V24       V25       V26       V27       V28  Class  \n",
      "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
      "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
      "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
      "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
      "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read CSV\n",
    "dataset = pd.read_csv(DATASET_NAME)\n",
    "\n",
    "assert not dataset.isnull().values.any()\n",
    "#dataset['Amount'] = StandardScaler().fit_transform(dataset[['Amount']])\n",
    "dataset = dataset.drop(['Amount'],axis=1)\n",
    "#dataset['Hour'] = dataset['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)\n",
    "dataset = dataset.drop(['Time'],axis=1)\n",
    "NUMBER_OF_FEATURES = dataset.shape[1] - 1 # Minus 1 because of column: 'Class'\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEXCAYAAACdwyIfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBY+IiFjq/wEs5BITJe0CZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1880f36be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histogram for all data\n",
    "count_classes = pd.value_counts(dataset['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "count_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset on train_and_validation dataset and test dataset\n",
    "# train_and_validation, test = split_data(dataset, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export train_and_validation and test dataset\n",
    "# pickle.dump( train_and_validation, open(os.path.join(\"..\", \"data\", \"train_and_validation_dataset.data\"), \"wb\" ))\n",
    "# pickle.dump( test, open(os.path.join(\"..\", \"data\", \"test_dataset.data\"), \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train_and_validation dataset\n",
    "train_and_validation = pickle.load(open(os.path.join(\"..\", \"data\", \"train_and_validation_dataset.data\"), \"rb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, neurons_number, learning_rate, activation):\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.output_layer = True\n",
    "\n",
    "        self.input = np.asmatrix(np.zeros((input_dim + 1, 1)))\n",
    "        self.output = np.asmatrix(np.zeros((neurons_number, 1)))\n",
    "        self.weights = np.asmatrix(np.random.uniform(low=-2/(input_dim**0.5), high=2/(input_dim**0.5), size=(input_dim + 1, neurons_number)))\n",
    "        self.deltas = np.asmatrix(np.zeros((neurons_number, 1)))\n",
    "        self.cumulative_gradient = np.asmatrix(np.zeros((input_dim + 1, neurons_number)))\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu(x)\n",
    "\n",
    "    def _get_gradient(self):\n",
    "        return np.matmul(self.input, self.deltas.transpose())\n",
    "\n",
    "    def forward_step(self, input_data):\n",
    "        self.input = np.concatenate([[[1]], input_data])  # Add bias\n",
    "        self.output = self._activate(np.matmul(self.weights.transpose(), self.input))\n",
    "        return self.output\n",
    "\n",
    "    def backward_step(self, next_weights=None, next_deltas=None, output_delta=None):\n",
    "        if self.output_layer:\n",
    "            self.deltas = output_delta\n",
    "        else:\n",
    "            derivative_of_activation = self.get_activation_derivative(self.output)\n",
    "            self.deltas = np.multiply(np.matmul(np.delete(next_weights, 0, 0), next_deltas),\n",
    "                                      derivative_of_activation)  # Exclude bias row from weights\n",
    "\n",
    "        self.cumulative_gradient = self.cumulative_gradient + self._get_gradient()\n",
    "\n",
    "    def get_activation_derivative(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid_derivative(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu_derivative(x)\n",
    "\n",
    "    def get_deltas(self):\n",
    "        return self.deltas\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, batch_size=50, epochs=20, loss='mse', regular_lambda=0.1):\n",
    "        self.layers = []\n",
    "        self.lerning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.regular_lambda = regular_lambda\n",
    "\n",
    "        self.training_history = []\n",
    "        self.validation_history = []\n",
    "\n",
    "    def _global_forward_step(self, x_train_batch):\n",
    "        y_predicted_batch = []\n",
    "        for x_train_record in x_train_batch:\n",
    "            x_train_record = x_train_record.reshape((-1, 1))\n",
    "            for layer in self.layers:\n",
    "                x_train_record = layer.forward_step(x_train_record)\n",
    "            y_predicted_batch.append(x_train_record)\n",
    "\n",
    "        return np.concatenate(y_predicted_batch)\n",
    "\n",
    "    def _global_backward_step(self, y_predicted_record, y_train_record):\n",
    "\n",
    "        output_delta = self._count_output_delta(y_predicted_record, y_train_record)\n",
    "\n",
    "        output_layer = self.layers[-1]\n",
    "        output_layer.backward_step(output_delta=output_delta)\n",
    "\n",
    "        next_deltas = output_layer.get_deltas()\n",
    "        next_weights = output_layer.get_weights()\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            layer.backward_step(next_weights=next_weights, next_deltas=next_deltas)\n",
    "\n",
    "    def _count_output_delta(self, y_predicted, y_actual):\n",
    "        if self.loss == 'mse':\n",
    "            return np.sum(np.multiply((y_predicted - y_actual), self.layers[-1].get_activation_derivative(y_predicted)), axis=0)\n",
    "\n",
    "    def _save_loss(self, x_tr, y_tr, x_val, y_val):\n",
    "        y_predicted_train = self._global_forward_step(x_tr)\n",
    "        train_loss = self._count_loss(y_predicted_train, y_tr)\n",
    "        self.training_history.append(train_loss)\n",
    "\n",
    "        if x_val is not None and y_val is not None:\n",
    "            y_predicted_validation = self._global_forward_step(x_val)\n",
    "            validation_loss = self._count_loss(y_predicted_validation, y_val)\n",
    "            self.validation_history.append(validation_loss)\n",
    "\n",
    "        print(\"Loss: \", train_loss, \" Accuracy: \", accuracy_score(y_tr, np.round(y_predicted_train)))\n",
    "\n",
    "    def _count_loss(self, y_predicted, y_actual):\n",
    "        if self.loss == 'mse':\n",
    "            return np.average(np.square(y_actual - y_predicted))\n",
    "\n",
    "    def add_layer(self, input_dim, neurons_number, activation='sigmoid'):\n",
    "        layer = Layer(input_dim, neurons_number, learning_rate=self.lerning_rate, activation=activation)\n",
    "        if self.layers:\n",
    "            self.layers[-1].output_layer = False\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None, distinct = 0):\n",
    "        print(x_train.shape, y_train.shape)\n",
    "        tr = np.hstack((x_train, y_train))\n",
    "        tr_f = tr[tr[:, -1] == 1]\n",
    "        tr_n = tr[tr[:, -1] == 0]\n",
    "        tr_f_ep = tr_f\n",
    "        tr_n_ep = tr_n\n",
    "        tr_score = np.concatenate((tr_f_ep, tr_n[: tr_f.shape[0],:]))\n",
    "        #print(\"FIT: tyle jest frauds:\", tr_f.shape)\n",
    "        #print(\"FIT: tyle jest normals:\", tr_n.shape)\n",
    "        for i in range(self.epochs):\n",
    "            if(distinct):\n",
    "                tr_n = np.roll(tr_n, tr_f.shape[0])\n",
    "                tr_n_ep = tr_n[: tr_f.shape[0],:]\n",
    "                tr = np.concatenate((tr_f_ep, tr_n_ep))\n",
    "                x_tr = tr[:,:-1]\n",
    "                y_tr = tr[:,-1:]\n",
    "                \n",
    "            else:\n",
    "                x_tr = x_train\n",
    "                y_tr = y_train\n",
    "                \n",
    "            # zeby w kazdej epoce byly inne dobre i te same zle ( w jednej walidacji oczywiscie)\n",
    "            \n",
    "            for idx in range(0, x_tr.shape[0], self.batch_size):\n",
    "                x_train_batch = x_tr[idx:idx + self.batch_size]\n",
    "                y_train_batch = y_tr[idx:idx + self.batch_size]\n",
    "\n",
    "                for n, x_train_record in enumerate(x_train_batch):\n",
    "                    x_train_record = x_train_record.reshape(1, -1)\n",
    "                    y_predicted_record = self._global_forward_step(x_train_record)\n",
    "                    self._global_backward_step(y_predicted_record, y_train_batch[n])\n",
    "\n",
    "                for lyr in self.layers:\n",
    "                    gradient = lyr.cumulative_gradient / x_train_batch.shape[0] + self.regular_lambda * lyr.weights\n",
    "                    lyr.weights = lyr.weights - np.multiply(self.lerning_rate, gradient)\n",
    "                    lyr.cumulative_gradient = np.asmatrix(np.zeros(lyr.cumulative_gradient.shape))\n",
    "            self._save_loss(tr_score[:,:-1], tr_score[:,-1:], x_val, y_val)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self._global_forward_step(x)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        y_predicted = self._global_forward_step(x)\n",
    "        loss = self._count_loss(y_predicted, y)\n",
    "\n",
    "        print(\"Loss: \", loss)\n",
    "        print(\"Accuracy: \", accuracy_score(y, np.round(y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.25539677e+00   2.12743727e-02  -1.13243701e-02 ...,  -2.68335728e-02\n",
      "   -9.33177553e-03   0.00000000e+00]\n",
      " [ -1.50582683e-01   1.18432064e+00  -3.17141240e-01 ...,   3.01912628e-01\n",
      "    1.23324180e-01   0.00000000e+00]\n",
      " [  2.27726062e+00  -1.61879223e+00  -2.36300252e+00 ...,   2.77170901e-02\n",
      "   -5.68606071e-02   0.00000000e+00]\n",
      " ..., \n",
      " [  1.46892767e+00  -1.01280055e+00  -1.74126684e+00 ...,  -7.00215686e-02\n",
      "   -4.72048908e-02   0.00000000e+00]\n",
      " [  1.24502282e+00  -6.03633716e-01   1.75631742e-01 ...,   2.43069783e-03\n",
      "    7.14548189e-03   0.00000000e+00]\n",
      " [ -8.24943982e+00   7.03180341e+00  -3.67406787e+00 ...,   9.56313339e-01\n",
      "    7.39704230e-01   0.00000000e+00]]\n",
      "(227065, 29) (390, 29) (227455, 29)\n"
     ]
    }
   ],
   "source": [
    "#Create array with only bad transactions for test&validation\n",
    "fraud_indices = np.array(train_and_validation[train_and_validation.Class == 1].index)\n",
    "fraud_indices = dataset.iloc[fraud_indices, :].sample(frac=1)\n",
    "fraud_indices = fraud_indices.values\n",
    "\n",
    "kfold_split_frauds = k_fold_split_data(fraud_indices, N_SPLITS)\n",
    "kfold_split_frauds_lens = [len(x) for x in kfold_split_frauds]\n",
    "\n",
    "normal_total = np.array(train_and_validation[train_and_validation.Class == 0].index)\n",
    "normal_total_df = dataset.iloc[normal_total, :].sample(frac=1)\n",
    "normal_total = normal_total_df.values\n",
    "print(normal_total)\n",
    "\n",
    "np.random.shuffle(normal_total)\n",
    "normal_validation_indices = normal_total[:fraud_indices.shape[0], :]\n",
    "normal_train = normal_total[fraud_indices.shape[0]:, :]\n",
    "\n",
    "kfold_split_normals = k_fold_split_data(normal_validation_indices, N_SPLITS)\n",
    "\n",
    "print(normal_train.shape, normal_validation_indices.shape, normal_total.shape)\n",
    "\n",
    "# k-fold validation with k=5\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.19397403134  Accuracy:  0.6330128205128205\n",
      "Loss:  0.181950937782  Accuracy:  0.6698717948717948\n",
      "Loss:  0.175251709843  Accuracy:  0.6842948717948718\n",
      "Loss:  0.168268983786  Accuracy:  0.7243589743589743\n",
      "Loss:  0.160243226023  Accuracy:  0.7483974358974359\n",
      "Loss:  0.154399963826  Accuracy:  0.7660256410256411\n",
      "Loss:  0.151246448645  Accuracy:  0.7724358974358975\n",
      "Loss:  0.14684246931  Accuracy:  0.782051282051282\n",
      "Loss:  0.142879128395  Accuracy:  0.7948717948717948\n",
      "Loss:  0.138532783981  Accuracy:  0.8060897435897436\n",
      "Loss:  0.134484133227  Accuracy:  0.8205128205128205\n",
      "Loss:  0.131618871449  Accuracy:  0.8301282051282052\n",
      "Loss:  0.12880090802  Accuracy:  0.842948717948718\n",
      "Loss:  0.125791088152  Accuracy:  0.8461538461538461\n",
      "Loss:  0.123882607662  Accuracy:  0.8509615384615384\n",
      "Loss:  0.121569196929  Accuracy:  0.8573717948717948\n",
      "Loss:  0.118976066684  Accuracy:  0.8589743589743589\n",
      "Loss:  0.117518479262  Accuracy:  0.8621794871794872\n",
      "Loss:  0.116117052755  Accuracy:  0.8653846153846154\n",
      "Loss:  0.114745946305  Accuracy:  0.8701923076923077\n",
      "Loss:  0.11239001519  Accuracy:  0.875\n",
      "Loss:  0.111193420943  Accuracy:  0.8782051282051282\n",
      "Loss:  0.109238583226  Accuracy:  0.8830128205128205\n",
      "Loss:  0.10770284344  Accuracy:  0.8878205128205128\n",
      "Loss:  0.106237620678  Accuracy:  0.8910256410256411\n",
      "Loss:  0.10491718275  Accuracy:  0.8926282051282052\n",
      "Loss:  0.103598900082  Accuracy:  0.8942307692307693\n",
      "Loss:  0.102644462566  Accuracy:  0.8958333333333334\n",
      "Loss:  0.101581582822  Accuracy:  0.8958333333333334\n",
      "Loss:  0.101032042839  Accuracy:  0.8958333333333334\n",
      "Loss:  0.100013013293  Accuracy:  0.8926282051282052\n",
      "Loss:  0.0991671768085  Accuracy:  0.8926282051282052\n",
      "Loss:  0.0986212113539  Accuracy:  0.8926282051282052\n",
      "Loss:  0.0980235684098  Accuracy:  0.8942307692307693\n",
      "Loss:  0.0976182373134  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0964040995789  Accuracy:  0.8942307692307693\n",
      "Loss:  0.0958352312061  Accuracy:  0.8942307692307693\n",
      "Loss:  0.0956021443845  Accuracy:  0.8942307692307693\n",
      "Loss:  0.0948579801867  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0942474315076  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0938184208071  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0931851935123  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0925126437842  Accuracy:  0.8990384615384616\n",
      "Loss:  0.091912664827  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0913178597138  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0913181827069  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0911742050173  Accuracy:  0.8990384615384616\n",
      "Loss:  0.0905793153837  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0902040304051  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0899952769225  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0897145121211  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0893123646338  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0888891846908  Accuracy:  0.9022435897435898\n",
      "Loss:  0.0882572378486  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0879720173008  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0874209179427  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0871582163044  Accuracy:  0.905448717948718\n",
      "Loss:  0.0868022089312  Accuracy:  0.905448717948718\n",
      "Loss:  0.086613033119  Accuracy:  0.905448717948718\n",
      "Loss:  0.0864194122006  Accuracy:  0.905448717948718\n",
      "Loss:  0.086094810138  Accuracy:  0.905448717948718\n",
      "Loss:  0.0858664903411  Accuracy:  0.905448717948718\n",
      "Loss:  0.0854148190178  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0850967875565  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0848636309677  Accuracy:  0.905448717948718\n",
      "Loss:  0.0845469517295  Accuracy:  0.9038461538461539\n",
      "Loss:  0.084490348168  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0841224653012  Accuracy:  0.9038461538461539\n",
      "Loss:  0.083942422197  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0837129696487  Accuracy:  0.905448717948718\n",
      "Loss:  0.0835462891753  Accuracy:  0.905448717948718\n",
      "Loss:  0.0832683473464  Accuracy:  0.905448717948718\n",
      "Loss:  0.0829820168803  Accuracy:  0.905448717948718\n",
      "Loss:  0.0828162336084  Accuracy:  0.907051282051282\n",
      "Loss:  0.0828951086898  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0827832814686  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0824574527726  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0822604790958  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0823378395234  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0821842930978  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0819594813075  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0817885043102  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0817106724829  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0816623805316  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0813705097304  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0810708620817  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0808334305543  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0806475333021  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0805200957208  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0804238052673  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0802740979183  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0801253611313  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0799541771772  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0798062873585  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0797636018602  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0796779062018  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0796254796229  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0794357113686  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0793203058639  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0791482828873  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0791110034433  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0789148836195  Accuracy:  0.9182692307692307\n",
      "Loss:  0.078778385022  Accuracy:  0.9182692307692307\n",
      "Loss:  0.078751869401  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0785543411962  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0783897497293  Accuracy:  0.9182692307692307\n",
      "Loss:  0.078268216711  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0783562893238  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0782516553022  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0781695912181  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0780137153638  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0778936239043  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0778828782041  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0776906477076  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0776208217551  Accuracy:  0.9182692307692307\n",
      "Loss:  0.077476124523  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0775039362419  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0774884219915  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0773734074638  Accuracy:  0.9198717948717948\n",
      "Loss:  0.077363043415  Accuracy:  0.9182692307692307\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0781836650434\n",
      "Accuracy:  0.9102564102564102\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.227379704846  Accuracy:  0.594551282051282\n",
      "Loss:  0.215148587908  Accuracy:  0.6137820512820513\n",
      "Loss:  0.20396327976  Accuracy:  0.6330128205128205\n",
      "Loss:  0.1940268065  Accuracy:  0.6474358974358975\n",
      "Loss:  0.183995334997  Accuracy:  0.6634615384615384\n",
      "Loss:  0.177554911665  Accuracy:  0.6666666666666666\n",
      "Loss:  0.169886722926  Accuracy:  0.6939102564102564\n",
      "Loss:  0.162709839599  Accuracy:  0.719551282051282\n",
      "Loss:  0.157251781139  Accuracy:  0.7307692307692307\n",
      "Loss:  0.151370639907  Accuracy:  0.7467948717948718\n",
      "Loss:  0.143934834756  Accuracy:  0.7708333333333334\n",
      "Loss:  0.137966052169  Accuracy:  0.7932692307692307\n",
      "Loss:  0.133221381278  Accuracy:  0.8044871794871795\n",
      "Loss:  0.129246176449  Accuracy:  0.8173076923076923\n",
      "Loss:  0.124981752542  Accuracy:  0.8285256410256411\n",
      "Loss:  0.120615235057  Accuracy:  0.8477564102564102\n",
      "Loss:  0.11781959743  Accuracy:  0.8525641025641025\n",
      "Loss:  0.115561739342  Accuracy:  0.8541666666666666\n",
      "Loss:  0.113024106642  Accuracy:  0.8653846153846154\n",
      "Loss:  0.110441760719  Accuracy:  0.8766025641025641\n",
      "Loss:  0.107426633835  Accuracy:  0.8846153846153846\n",
      "Loss:  0.105801456363  Accuracy:  0.8990384615384616\n",
      "Loss:  0.103324231086  Accuracy:  0.9022435897435898\n",
      "Loss:  0.100710603823  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0987894378055  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0974408938645  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0960904715516  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0947467455484  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0930051049949  Accuracy:  0.9182692307692307\n",
      "Loss:  0.091875045615  Accuracy:  0.9198717948717948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0901866061936  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0892401937203  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0881700636998  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0870899609373  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0858318432081  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0851532167816  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0844010187262  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0839668395141  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0832492477238  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0823955019611  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0817533945491  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0808750816411  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0800958291862  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0794468420114  Accuracy:  0.9326923076923077\n",
      "Loss:  0.0787973947094  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0787515146434  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0782487622805  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0777209155629  Accuracy:  0.9391025641025641\n",
      "Loss:  0.077370890202  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0769062796328  Accuracy:  0.9391025641025641\n",
      "Loss:  0.076295654781  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0760036002075  Accuracy:  0.9375\n",
      "Loss:  0.075602305335  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0749227113488  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0743544881039  Accuracy:  0.9423076923076923\n",
      "Loss:  0.073907505166  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0736009279974  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0732580791182  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0728055879318  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0723788025075  Accuracy:  0.9423076923076923\n",
      "Loss:  0.072077273886  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0718429820555  Accuracy:  0.9407051282051282\n",
      "Loss:  0.071683405723  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0714500955945  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0714240590567  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0713022219845  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0711552776296  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0710567336162  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0707321966137  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0705875947644  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0705392043698  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0701955233852  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0700401747854  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0698502394316  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0698741227453  Accuracy:  0.9439102564102564\n",
      "Loss:  0.070123412177  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0698500098865  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0697790133291  Accuracy:  0.9439102564102564\n",
      "Loss:  0.06935070072  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0691164931518  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0689972770143  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0687922229649  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0684332596596  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0683365178737  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0682866245044  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0682340444505  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0681124233913  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0678791309517  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0677819227759  Accuracy:  0.9439102564102564\n",
      "Loss:  0.067701483156  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0676240415234  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0676480418639  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0675902661999  Accuracy:  0.9423076923076923\n",
      "Loss:  0.067540924218  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0675180353034  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0674220128742  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0673470309658  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0672794390677  Accuracy:  0.9439102564102564\n",
      "Loss:  0.0672659204364  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0672037267725  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0671008453361  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0669815343533  Accuracy:  0.9423076923076923\n",
      "Loss:  0.066873403726  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0668737256707  Accuracy:  0.9423076923076923\n",
      "Loss:  0.0669121896529  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0668780983503  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0668045970408  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0668832974985  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0668190592726  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0667848685832  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0668428631618  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0667873567713  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0669205869677  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0669521435885  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0670123400694  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0669519501615  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0667763068177  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0668427980368  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0668028674547  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0667924051025  Accuracy:  0.9391025641025641\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0827533876653\n",
      "Accuracy:  0.9102564102564102\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.172371693935  Accuracy:  0.7612179487179487\n",
      "Loss:  0.157517678976  Accuracy:  0.7676282051282052\n",
      "Loss:  0.150124889134  Accuracy:  0.7884615384615384\n",
      "Loss:  0.144404391181  Accuracy:  0.8092948717948718\n",
      "Loss:  0.14038683066  Accuracy:  0.8125\n",
      "Loss:  0.136508040845  Accuracy:  0.8269230769230769\n",
      "Loss:  0.133182832816  Accuracy:  0.844551282051282\n",
      "Loss:  0.129853387472  Accuracy:  0.8509615384615384\n",
      "Loss:  0.127384877288  Accuracy:  0.8573717948717948\n",
      "Loss:  0.124642218  Accuracy:  0.8669871794871795\n",
      "Loss:  0.12267912421  Accuracy:  0.8701923076923077\n",
      "Loss:  0.120385175163  Accuracy:  0.8798076923076923\n",
      "Loss:  0.118243576823  Accuracy:  0.8814102564102564\n",
      "Loss:  0.116271626896  Accuracy:  0.8846153846153846\n",
      "Loss:  0.11415022403  Accuracy:  0.8878205128205128\n",
      "Loss:  0.112563104465  Accuracy:  0.8878205128205128\n",
      "Loss:  0.110868628357  Accuracy:  0.8894230769230769\n",
      "Loss:  0.109393897022  Accuracy:  0.8942307692307693\n",
      "Loss:  0.108247097726  Accuracy:  0.8958333333333334\n",
      "Loss:  0.106933564118  Accuracy:  0.8974358974358975\n",
      "Loss:  0.105163884876  Accuracy:  0.9006410256410257\n",
      "Loss:  0.103670376087  Accuracy:  0.9006410256410257\n",
      "Loss:  0.102457237734  Accuracy:  0.8990384615384616\n",
      "Loss:  0.101427402706  Accuracy:  0.8990384615384616\n",
      "Loss:  0.100448629255  Accuracy:  0.9006410256410257\n",
      "Loss:  0.0993648963024  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0982286729058  Accuracy:  0.9086538461538461\n",
      "Loss:  0.0972309260294  Accuracy:  0.9134615384615384\n",
      "Loss:  0.096286767734  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0955080824881  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0947293154974  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0941280852249  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0934696984844  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0929320044488  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0921170147753  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0914228789829  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0908003948796  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0901584726081  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0894938023402  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0887007053571  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0877424820084  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0858910820239  Accuracy:  0.9166666666666666\n",
      "Loss:  0.082488932819  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0787597809613  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0778285010627  Accuracy:  0.9278846153846154\n",
      "Loss:  0.077239396473  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0763036103624  Accuracy:  0.9294871794871795\n",
      "Loss:  0.075689207366  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0752674990181  Accuracy:  0.9310897435897436\n",
      "Loss:  0.0746753773563  Accuracy:  0.9342948717948718\n",
      "Loss:  0.074205692989  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0736309278758  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0731685744479  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0728189848371  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0726225627604  Accuracy:  0.9342948717948718\n",
      "Loss:  0.072286249395  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0720224966547  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0717373361788  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0715985397549  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0715298666154  Accuracy:  0.9358974358974359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0714584790525  Accuracy:  0.9358974358974359\n",
      "Loss:  0.071259485591  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0708580041746  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0706646212885  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0705657064623  Accuracy:  0.9358974358974359\n",
      "Loss:  0.070465340969  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0703339994431  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0701476832441  Accuracy:  0.9358974358974359\n",
      "Loss:  0.069994062688  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0699004891696  Accuracy:  0.9375\n",
      "Loss:  0.0697212856237  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0696188900392  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0694892024149  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0693518427102  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0693629450523  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0693072661196  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0691841007565  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0690412228071  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0688984512463  Accuracy:  0.9375\n",
      "Loss:  0.0688195581825  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0687694132266  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0686253629934  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0685460318649  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0685155260529  Accuracy:  0.9375\n",
      "Loss:  0.0684183581855  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0682942032298  Accuracy:  0.9375\n",
      "Loss:  0.0681897613425  Accuracy:  0.9375\n",
      "Loss:  0.0681201998342  Accuracy:  0.9375\n",
      "Loss:  0.0680571576071  Accuracy:  0.9375\n",
      "Loss:  0.0680235604423  Accuracy:  0.9375\n",
      "Loss:  0.0680199470728  Accuracy:  0.9375\n",
      "Loss:  0.0679829354782  Accuracy:  0.9375\n",
      "Loss:  0.0678458927953  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0677063182069  Accuracy:  0.9375\n",
      "Loss:  0.067723880324  Accuracy:  0.9375\n",
      "Loss:  0.0677866239334  Accuracy:  0.9375\n",
      "Loss:  0.0676982535748  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0676145744895  Accuracy:  0.9375\n",
      "Loss:  0.0676151733269  Accuracy:  0.9375\n",
      "Loss:  0.0676223738146  Accuracy:  0.9375\n",
      "Loss:  0.0676025840923  Accuracy:  0.9375\n",
      "Loss:  0.067507752785  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0674791529815  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0675171761007  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0674888449442  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0675464137258  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0675050043991  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0673814823261  Accuracy:  0.9375\n",
      "Loss:  0.0673074535837  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0672587951812  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0672081429169  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0673285336543  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0672871775859  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0672725542688  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0671629573723  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0671112680202  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0671632665244  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0672093746962  Accuracy:  0.9407051282051282\n",
      "Loss:  0.0672073713634  Accuracy:  0.9407051282051282\n",
      "Loss:  0.067202314726  Accuracy:  0.9407051282051282\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0630170347549\n",
      "Accuracy:  0.9358974358974359\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.20246255098  Accuracy:  0.6602564102564102\n",
      "Loss:  0.190198117865  Accuracy:  0.6858974358974359\n",
      "Loss:  0.182462362755  Accuracy:  0.7003205128205128\n",
      "Loss:  0.173161038288  Accuracy:  0.7147435897435898\n",
      "Loss:  0.165581664971  Accuracy:  0.7291666666666666\n",
      "Loss:  0.158807772188  Accuracy:  0.75\n",
      "Loss:  0.151604575331  Accuracy:  0.7644230769230769\n",
      "Loss:  0.145356585077  Accuracy:  0.782051282051282\n",
      "Loss:  0.139662057811  Accuracy:  0.7964743589743589\n",
      "Loss:  0.136802383378  Accuracy:  0.7996794871794872\n",
      "Loss:  0.132305635169  Accuracy:  0.8125\n",
      "Loss:  0.128067898776  Accuracy:  0.8173076923076923\n",
      "Loss:  0.125770315095  Accuracy:  0.8173076923076923\n",
      "Loss:  0.120924534483  Accuracy:  0.8333333333333334\n",
      "Loss:  0.118341151801  Accuracy:  0.8349358974358975\n",
      "Loss:  0.1161580801  Accuracy:  0.844551282051282\n",
      "Loss:  0.113378805533  Accuracy:  0.8541666666666666\n",
      "Loss:  0.11087736691  Accuracy:  0.8573717948717948\n",
      "Loss:  0.10919730656  Accuracy:  0.8621794871794872\n",
      "Loss:  0.107242195418  Accuracy:  0.8717948717948718\n",
      "Loss:  0.105621623655  Accuracy:  0.8782051282051282\n",
      "Loss:  0.104152480116  Accuracy:  0.8814102564102564\n",
      "Loss:  0.10242926228  Accuracy:  0.8846153846153846\n",
      "Loss:  0.100829147079  Accuracy:  0.8894230769230769\n",
      "Loss:  0.0994025708743  Accuracy:  0.8958333333333334\n",
      "Loss:  0.0979725132668  Accuracy:  0.8974358974358975\n",
      "Loss:  0.0963319629264  Accuracy:  0.905448717948718\n",
      "Loss:  0.0948560360329  Accuracy:  0.9102564102564102\n",
      "Loss:  0.093626921174  Accuracy:  0.9134615384615384\n",
      "Loss:  0.092938462917  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0920142792  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0912048085708  Accuracy:  0.9150641025641025\n",
      "Loss:  0.090454830084  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0899318814893  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0896579533569  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0886842106442  Accuracy:  0.9166666666666666\n",
      "Loss:  0.087909543429  Accuracy:  0.9166666666666666\n",
      "Loss:  0.087200608206  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0865316858664  Accuracy:  0.9150641025641025\n",
      "Loss:  0.0860706141006  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0854996664516  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0849290093404  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0843623040384  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0837347597185  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0833898574105  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0830245183875  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0825882435104  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0821592623139  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0817256228454  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0813528653042  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0807816956554  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0804035496987  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0801115063731  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0799580042046  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0796929626559  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0793362240229  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0791617548058  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0788164711468  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0785832796522  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0784307726948  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0782987810554  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0780695999795  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0778031472265  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0774528162468  Accuracy:  0.9230769230769231\n",
      "Loss:  0.077208992866  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0770185842539  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0769992304208  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0767897341328  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0765218584639  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0763664380778  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0763081395079  Accuracy:  0.9230769230769231\n",
      "Loss:  0.076198538811  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0761153824028  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0758753378294  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0758071049451  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0756935904037  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0757304707481  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0756339207367  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0753996278238  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0751195032805  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0749126487613  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0748363351954  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0748990607314  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0747763526884  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0745948088134  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0745491731703  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0744235524037  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0743760741022  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0741501091012  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0740696414432  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0739819319468  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0740114015693  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0739217636561  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0738912449024  Accuracy:  0.9278846153846154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0737961261119  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0738978186155  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0737789432826  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0738161675203  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0737405394758  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0736049518691  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0734878701228  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0733942736331  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0732843427133  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0732785340753  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0732049600779  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0731662838494  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0731156656802  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0730999279139  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0731078705541  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0730815444824  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0730428069337  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729947665115  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0729953940734  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729590176768  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0730093480388  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729320052122  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0728880400791  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729134193041  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729331337943  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0729226833171  Accuracy:  0.9294871794871795\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0709070856726\n",
      "Accuracy:  0.9166666666666666\n",
      "\n",
      "\n",
      "312\n",
      "(37752, 28) (37752, 1)\n",
      "Loss:  0.621138947075  Accuracy:  0.15224358974358973\n",
      "Loss:  0.596421576305  Accuracy:  0.16666666666666666\n",
      "Loss:  0.530618250105  Accuracy:  0.22115384615384615\n",
      "Loss:  0.401325746109  Accuracy:  0.40064102564102566\n",
      "Loss:  0.347135206146  Accuracy:  0.4775641025641026\n",
      "Loss:  0.324533565092  Accuracy:  0.5016025641025641\n",
      "Loss:  0.308445757321  Accuracy:  0.5176282051282052\n",
      "Loss:  0.294659306933  Accuracy:  0.5256410256410257\n",
      "Loss:  0.285975601489  Accuracy:  0.5336538461538461\n",
      "Loss:  0.274876944459  Accuracy:  0.5336538461538461\n",
      "Loss:  0.263376105084  Accuracy:  0.5288461538461539\n",
      "Loss:  0.253125910049  Accuracy:  0.5432692307692307\n",
      "Loss:  0.243023100736  Accuracy:  0.5464743589743589\n",
      "Loss:  0.235438176494  Accuracy:  0.5512820512820513\n",
      "Loss:  0.224130731588  Accuracy:  0.5689102564102564\n",
      "Loss:  0.214693066883  Accuracy:  0.5769230769230769\n",
      "Loss:  0.205254058553  Accuracy:  0.5833333333333334\n",
      "Loss:  0.199702893208  Accuracy:  0.592948717948718\n",
      "Loss:  0.19096380961  Accuracy:  0.6057692307692307\n",
      "Loss:  0.184255832325  Accuracy:  0.6169871794871795\n",
      "Loss:  0.174923540741  Accuracy:  0.6538461538461539\n",
      "Loss:  0.169946406107  Accuracy:  0.6586538461538461\n",
      "Loss:  0.164510192805  Accuracy:  0.6891025641025641\n",
      "Loss:  0.158850546091  Accuracy:  0.7115384615384616\n",
      "Loss:  0.154414979016  Accuracy:  0.7291666666666666\n",
      "Loss:  0.150601606454  Accuracy:  0.7451923076923077\n",
      "Loss:  0.147493284946  Accuracy:  0.75\n",
      "Loss:  0.143731553391  Accuracy:  0.782051282051282\n",
      "Loss:  0.138982289816  Accuracy:  0.7948717948717948\n",
      "Loss:  0.136272124457  Accuracy:  0.8076923076923077\n",
      "Loss:  0.132949019341  Accuracy:  0.8205128205128205\n",
      "Loss:  0.129810028585  Accuracy:  0.8381410256410257\n",
      "Loss:  0.126366112085  Accuracy:  0.8605769230769231\n",
      "Loss:  0.12297390154  Accuracy:  0.8782051282051282\n",
      "Loss:  0.120456099183  Accuracy:  0.8862179487179487\n",
      "Loss:  0.118356883199  Accuracy:  0.8894230769230769\n",
      "Loss:  0.115180967006  Accuracy:  0.8974358974358975\n",
      "Loss:  0.11323567392  Accuracy:  0.8974358974358975\n",
      "Loss:  0.110936433542  Accuracy:  0.9038461538461539\n",
      "Loss:  0.109366893758  Accuracy:  0.907051282051282\n",
      "Loss:  0.107852728385  Accuracy:  0.907051282051282\n",
      "Loss:  0.106428826813  Accuracy:  0.9118589743589743\n",
      "Loss:  0.104743831533  Accuracy:  0.9134615384615384\n",
      "Loss:  0.10325392836  Accuracy:  0.9166666666666666\n",
      "Loss:  0.101989780405  Accuracy:  0.9182692307692307\n",
      "Loss:  0.100705202522  Accuracy:  0.9118589743589743\n",
      "Loss:  0.0996194950469  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0985693815214  Accuracy:  0.9134615384615384\n",
      "Loss:  0.0976196321699  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0961411765361  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0951967327535  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0943705939604  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0936841658986  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0929213045183  Accuracy:  0.9182692307692307\n",
      "Loss:  0.0923095076692  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0917196363181  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0909809918019  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0902551841314  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0892933491702  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0885225842822  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0880603373302  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0876562875883  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0872289386756  Accuracy:  0.9198717948717948\n",
      "Loss:  0.0869033967422  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0864370265173  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0861214749321  Accuracy:  0.9214743589743589\n",
      "Loss:  0.0862110201579  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0858746965641  Accuracy:  0.9230769230769231\n",
      "Loss:  0.085562656494  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0852361067105  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0850056494229  Accuracy:  0.9230769230769231\n",
      "Loss:  0.084675220415  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0843307666913  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0841932412386  Accuracy:  0.9246794871794872\n",
      "Loss:  0.0840440103119  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0839652577752  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0837121202245  Accuracy:  0.9278846153846154\n",
      "Loss:  0.083575059938  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0833090708706  Accuracy:  0.9278846153846154\n",
      "Loss:  0.083150499799  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0829088703485  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0826791449583  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0827130932593  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0826862905628  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0822578532741  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0819596326881  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0816395711221  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0814557074356  Accuracy:  0.9278846153846154\n",
      "Loss:  0.081494346879  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0813199445329  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0811198846113  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0809877922095  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0806477873086  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0804048583809  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0802647085515  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0802522789204  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0799955413043  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0798057739693  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0795954145319  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0793647640169  Accuracy:  0.9262820512820513\n",
      "Loss:  0.0788649743731  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0785370815033  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0781344506397  Accuracy:  0.9278846153846154\n",
      "Loss:  0.077818635784  Accuracy:  0.9278846153846154\n",
      "Loss:  0.077198488897  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0764748850388  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0756869838417  Accuracy:  0.9278846153846154\n",
      "Loss:  0.0750934188662  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0748417278842  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0744670461841  Accuracy:  0.9342948717948718\n",
      "Loss:  0.0742454773435  Accuracy:  0.9358974358974359\n",
      "Loss:  0.0746080462805  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0745517794426  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0742819800198  Accuracy:  0.9391025641025641\n",
      "Loss:  0.074045075644  Accuracy:  0.9391025641025641\n",
      "Loss:  0.0737794599882  Accuracy:  0.9375\n",
      "Loss:  0.0738277527179  Accuracy:  0.9375\n",
      "Loss:  0.073993756272  Accuracy:  0.9391025641025641\n",
      "Loss:  0.073830682723  Accuracy:  0.9375\n",
      "Loss:  0.073759315847  Accuracy:  0.9375\n",
      "\n",
      " ================================================================================\n",
      "\n",
      "Validation dataset evaluation:\n",
      "Loss:  0.0743808767743\n",
      "Accuracy:  0.9423076923076923\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, validation_frauds in enumerate(kfold_split_frauds):\n",
    "    #Get train frauds from train&validation frauds\n",
    "    train_frauds = kfold_split_frauds.copy()\n",
    "    train_frauds.pop(k)\n",
    "    train_frauds = np.concatenate(train_frauds)\n",
    "    print(len(train_frauds))\n",
    "    \n",
    "    validation_normals = kfold_split_normals[k]\n",
    "    np.random.shuffle(normal_train)\n",
    "    train_normals = normal_train[:train_frauds.shape[0]*EPOCHS,:]\n",
    "    \n",
    "    train = np.concatenate([train_frauds, train_normals])\n",
    "    \n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1:]\n",
    "    \n",
    "    validation = np.concatenate([validation_frauds,validation_normals])\n",
    "    np.random.shuffle(validation)\n",
    "\n",
    "    x_validation = validation[:, :-1]\n",
    "    y_validation = validation[:, -1:]\n",
    "\n",
    "    model = NeuralNetwork(learning_rate=LEARNING_RATE, batch_size=x_train.shape[0], epochs=EPOCHS, loss='mse', regular_lambda=REGULARIZATION_LAMBDA)\n",
    "    model.add_layer(input_dim=NUMBER_OF_FEATURES, neurons_number=NUMBER_OF_NEURONS, activation='relu')\n",
    "    model.add_layer(input_dim=NUMBER_OF_NEURONS, neurons_number=1, activation='sigmoid')\n",
    "    model.fit(x_train, y_train, x_validation, y_validation, distinct=1)\n",
    "\n",
    "    print(\"\\n\", '='*80)\n",
    "    print(\"\\nValidation dataset evaluation:\")\n",
    "    model.evaluate(x_validation, y_validation)\n",
    "    print(\"\\n\")\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEaCAYAAADpMdsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEKIgknyEEIIUbB/ACjdCUEZNdfeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f187dadfda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_history = np.average([mdl.training_history for mdl in models], axis=0)\n",
    "validation_history = np.average([mdl.validation_history for mdl in models], axis=0)\n",
    "\n",
    "plot_loss(EPOCHS, training_history, validation_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION ON TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset evaluation:\n"
     ]
    }
   ],
   "source": [
    "# Read test dataset\n",
    "test = pickle.load(open(os.path.join(\"..\", \"data\", \"test_dataset.data\"), \"rb\" ))\n",
    "\n",
    "# Convert test data to numpyarray and split them.\n",
    "test = test.values\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1:]\n",
    "\n",
    "print(\"\\nTest dataset evaluation:\")\n",
    "train_and_validation = train_and_validation.values \n",
    "x_train_and_validation = train_and_validation[:, :-1]\n",
    "y_train_and_validation = train_and_validation[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 28) (227845, 1)\n",
      "Loss:  0.242230535893  Accuracy:  0.6435897435897436\n",
      "Loss:  0.172263122563  Accuracy:  0.7051282051282052\n",
      "Loss:  0.165081684438  Accuracy:  0.7217948717948718\n",
      "Loss:  0.158889719233  Accuracy:  0.7397435897435898\n",
      "Loss:  0.153365466172  Accuracy:  0.7551282051282051\n",
      "Loss:  0.148007005775  Accuracy:  0.7705128205128206\n",
      "Loss:  0.142644360687  Accuracy:  0.7769230769230769\n",
      "Loss:  0.137711753579  Accuracy:  0.7935897435897435\n",
      "Loss:  0.133987862158  Accuracy:  0.8025641025641026\n",
      "Loss:  0.129764389389  Accuracy:  0.8179487179487179\n",
      "Loss:  0.125691198951  Accuracy:  0.8282051282051283\n",
      "Loss:  0.122339795275  Accuracy:  0.8358974358974359\n",
      "Loss:  0.120104770413  Accuracy:  0.8448717948717949\n",
      "Loss:  0.117605999006  Accuracy:  0.8551282051282051\n",
      "Loss:  0.115179986901  Accuracy:  0.8615384615384616\n",
      "Loss:  0.113083337982  Accuracy:  0.8628205128205129\n",
      "Loss:  0.110365250556  Accuracy:  0.867948717948718\n",
      "Loss:  0.108240841968  Accuracy:  0.8743589743589744\n",
      "Loss:  0.105971869032  Accuracy:  0.8782051282051282\n",
      "Loss:  0.10459714147  Accuracy:  0.8833333333333333\n",
      "Loss:  0.103375245375  Accuracy:  0.8871794871794871\n",
      "Loss:  0.101467268224  Accuracy:  0.8948717948717949\n",
      "Loss:  0.0999477700342  Accuracy:  0.9012820512820513\n",
      "Loss:  0.0987754097793  Accuracy:  0.9012820512820513\n",
      "Loss:  0.0974361966073  Accuracy:  0.9038461538461539\n",
      "Loss:  0.0959931092901  Accuracy:  0.9064102564102564\n",
      "Loss:  0.0951823055832  Accuracy:  0.9064102564102564\n",
      "Loss:  0.094240589422  Accuracy:  0.9102564102564102\n",
      "Loss:  0.0931665755435  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0925286256149  Accuracy:  0.9115384615384615\n",
      "Loss:  0.0912911971678  Accuracy:  0.9153846153846154\n",
      "Loss:  0.0907222586257  Accuracy:  0.9128205128205128\n",
      "Loss:  0.0899039018444  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0892415813313  Accuracy:  0.9153846153846154\n",
      "Loss:  0.0883288561576  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0875944420297  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0868425549184  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0863067376054  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0856330065906  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0850027385863  Accuracy:  0.9141025641025641\n",
      "Loss:  0.0843140349744  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0837574254784  Accuracy:  0.9166666666666666\n",
      "Loss:  0.0832112451281  Accuracy:  0.9179487179487179\n",
      "Loss:  0.0827620957769  Accuracy:  0.9192307692307692\n",
      "Loss:  0.0823349020129  Accuracy:  0.9192307692307692\n",
      "Loss:  0.0819560047324  Accuracy:  0.9205128205128205\n",
      "Loss:  0.0815214006299  Accuracy:  0.9217948717948717\n",
      "Loss:  0.081134604617  Accuracy:  0.9205128205128205\n",
      "Loss:  0.0812238988621  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0809034263477  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0807418333525  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0807296485392  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0797590201845  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0791653104654  Accuracy:  0.9230769230769231\n",
      "Loss:  0.0788030915117  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0785709297378  Accuracy:  0.9217948717948717\n",
      "Loss:  0.0781764931824  Accuracy:  0.9243589743589744\n",
      "Loss:  0.0778133502448  Accuracy:  0.9256410256410257\n",
      "Loss:  0.077588764366  Accuracy:  0.9256410256410257\n",
      "Loss:  0.0773066965885  Accuracy:  0.9256410256410257\n",
      "Loss:  0.0771022214332  Accuracy:  0.926923076923077\n",
      "Loss:  0.0768319896691  Accuracy:  0.926923076923077\n",
      "Loss:  0.076553578622  Accuracy:  0.926923076923077\n",
      "Loss:  0.0763305370508  Accuracy:  0.926923076923077\n",
      "Loss:  0.0761664233472  Accuracy:  0.926923076923077\n",
      "Loss:  0.0759442176234  Accuracy:  0.926923076923077\n",
      "Loss:  0.0757653614621  Accuracy:  0.9256410256410257\n",
      "Loss:  0.0755828718267  Accuracy:  0.9256410256410257\n",
      "Loss:  0.0755161677588  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0753814382099  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0752342650312  Accuracy:  0.926923076923077\n",
      "Loss:  0.0751349705782  Accuracy:  0.926923076923077\n",
      "Loss:  0.0750096936695  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0747914622298  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0747036060772  Accuracy:  0.9282051282051282\n",
      "Loss:  0.074537994563  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0743925449504  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0740601936603  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0738898029514  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0737088579615  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0734844495374  Accuracy:  0.9282051282051282\n",
      "Loss:  0.0733524497019  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0731966403583  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0731513435045  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0730717456947  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0730487818378  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0728949328016  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0728600055063  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0727782757969  Accuracy:  0.9307692307692308\n",
      "Loss:  0.072713232393  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0727869950807  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0727320371975  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0725338553395  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0724822636164  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0723274625319  Accuracy:  0.9307692307692308\n",
      "Loss:  0.072247066143  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0720916134406  Accuracy:  0.9307692307692308\n",
      "Loss:  0.072021378192  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0718935830794  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0718746272959  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0718214894369  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0717311899466  Accuracy:  0.9294871794871795\n",
      "Loss:  0.0716525178655  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0716400120403  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0715342349915  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0715331458286  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0715688727238  Accuracy:  0.9346153846153846\n",
      "Loss:  0.0714312192507  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0713880723511  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0713286597999  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0713682236262  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0712934188872  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0712702477149  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0712384103227  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0711561032922  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0710752567369  Accuracy:  0.9320512820512821\n",
      "Loss:  0.0710187382585  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0709843798191  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0709558904644  Accuracy:  0.9307692307692308\n",
      "Loss:  0.0709484311637  Accuracy:  0.9307692307692308\n"
     ]
    }
   ],
   "source": [
    "test_model = NeuralNetwork(learning_rate=LEARNING_RATE, batch_size=x_train_and_validation.shape[0], epochs=EPOCHS, loss='mse', regular_lambda=REGULARIZATION_LAMBDA)\n",
    "test_model.add_layer(input_dim=NUMBER_OF_FEATURES, neurons_number=NUMBER_OF_NEURONS, activation='relu')\n",
    "test_model.add_layer(input_dim=NUMBER_OF_NEURONS, neurons_number=1, activation='sigmoid')\n",
    "test_model.fit(x_train_and_validation, y_train_and_validation, distinct=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0698601400928\n",
      "Accuracy:  0.9791439907306625\n",
      "\n",
      "Precision: 0.056220095693779906\n",
      "Recall: 0.9215686274509803\n",
      "F-score: 0.10597519729425027\n",
      "\n",
      "\n",
      "             actual 1  actual 0\n",
      "predicted 1        94      1578\n",
      "predicted 0         8     55282\n"
     ]
    }
   ],
   "source": [
    "test_model.evaluate(x_test, y_test)\n",
    "plot_confusion_matrix(model, x_test, y_test)\n",
    "# print(model.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [02:36<00:00,  7.44s/it]\n",
      "/home/maciejpesko/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEaCAYAAADtxAsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQwmaSNIQQQtjs/wHzkKmT+hgHdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1880f36cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TPR_array = []\n",
    "FPR_array = []\n",
    "for i in tqdm(range(-50,55,5)):\n",
    "    predicted = np.round(model.predict(x_test) + i/100).reshape(y_test.shape)\n",
    "    actual = y_test  \n",
    "    TP = np.count_nonzero(np.multiply(predicted, actual))\n",
    "    TN = np.count_nonzero(np.multiply(predicted - 1, actual - 1))\n",
    "    FP = np.count_nonzero(np.multiply(predicted, actual - 1))\n",
    "    FN = np.count_nonzero(np.multiply(predicted - 1, actual))\n",
    "\n",
    "    TPR_array.append(TP / (TP+FN))\n",
    "    FPR_array.append(FP / (FP+TN))\n",
    "\n",
    "plot_ROC(TPR_array, FPR_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
